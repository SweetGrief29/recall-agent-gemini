{"version":3,"sources":["../../../node_modules/.pnpm/@ai-sdk+openai-compatible@1.0.7_zod@3.25.76/node_modules/@ai-sdk/openai-compatible/src/openai-compatible-chat-options.ts","../../../node_modules/.pnpm/@ai-sdk+openai-compatible@1.0.7_zod@3.25.76/node_modules/@ai-sdk/openai-compatible/src/openai-compatible-error.ts","../../../node_modules/.pnpm/@ai-sdk+openai-compatible@1.0.7_zod@3.25.76/node_modules/@ai-sdk/openai-compatible/src/openai-compatible-chat-language-model.ts","../../../node_modules/.pnpm/@ai-sdk+openai-compatible@1.0.7_zod@3.25.76/node_modules/@ai-sdk/openai-compatible/src/openai-compatible-completion-options.ts","../../../node_modules/.pnpm/@ai-sdk+openai-compatible@1.0.7_zod@3.25.76/node_modules/@ai-sdk/openai-compatible/src/openai-compatible-completion-language-model.ts","../../../node_modules/.pnpm/@ai-sdk+openai-compatible@1.0.7_zod@3.25.76/node_modules/@ai-sdk/openai-compatible/src/openai-compatible-embedding-options.ts","../../../node_modules/.pnpm/@ai-sdk+openai-compatible@1.0.7_zod@3.25.76/node_modules/@ai-sdk/openai-compatible/src/openai-compatible-embedding-model.ts","../../../node_modules/.pnpm/@ai-sdk+openai-compatible@1.0.7_zod@3.25.76/node_modules/@ai-sdk/openai-compatible/src/openai-compatible-image-model.ts","../../../node_modules/.pnpm/@ai-sdk+xai@2.0.7_zod@3.25.76/node_modules/@ai-sdk/xai/src/convert-to-xai-chat-messages.ts","../../../node_modules/.pnpm/@ai-sdk+xai@2.0.7_zod@3.25.76/node_modules/@ai-sdk/xai/src/get-response-metadata.ts","../../../node_modules/.pnpm/@ai-sdk+xai@2.0.7_zod@3.25.76/node_modules/@ai-sdk/xai/src/map-xai-finish-reason.ts","../../../node_modules/.pnpm/@ai-sdk+xai@2.0.7_zod@3.25.76/node_modules/@ai-sdk/xai/src/xai-chat-options.ts","../../../node_modules/.pnpm/@ai-sdk+xai@2.0.7_zod@3.25.76/node_modules/@ai-sdk/xai/src/xai-error.ts","../../../node_modules/.pnpm/@ai-sdk+xai@2.0.7_zod@3.25.76/node_modules/@ai-sdk/xai/src/xai-prepare-tools.ts","../../../node_modules/.pnpm/@ai-sdk+xai@2.0.7_zod@3.25.76/node_modules/@ai-sdk/xai/src/xai-chat-language-model.ts","../../../node_modules/.pnpm/@ai-sdk+xai@2.0.7_zod@3.25.76/node_modules/@ai-sdk/xai/src/xai-provider.ts"],"names":["z","postJsonToApi","combineHeaders","createJsonErrorResponseHandler","createJsonResponseHandler","UnsupportedFunctionalityError","_a"],"mappings":";;;AAI+C,EAAE,MAAA,CAAO;;;;;EAKtD,IAAA,EAAM,CAAA,CAAE,MAAA,EAAO,CAAE,QAAA,EAAS;;;;EAK1B,eAAA,EAAiB,CAAA,CAAE,MAAA,EAAO,CAAE,QAAA;AAC9B,CAAC;ACbM,IAAM,+BAAA,GAAkCA,EAAE,MAAA,CAAO;AACtD,EAAA,KAAA,EAAOA,EAAE,MAAA,CAAO;AACd,IAAA,OAAA,EAASA,EAAE,MAAA,EAAO;;;;IAKlB,IAAA,EAAMA,CAAAA,CAAE,MAAA,EAAO,CAAE,OAAA,EAAQ;IACzB,KAAA,EAAOA,CAAAA,CAAE,GAAA,EAAI,CAAE,OAAA,EAAQ;IACvB,IAAA,EAAMA,CAAAA,CAAE,KAAA,CAAM,CAACA,CAAAA,CAAE,MAAA,EAAO,EAAGA,CAAAA,CAAE,MAAA,EAAQ,CAAC,CAAA,CAAE,OAAA;GACzC;AACH,CAAC,CAAA;AAYM,IAAM,qCAAA,GACX;EACE,WAAA,EAAa,+BAAA;EACb,cAAA,EAAgB,CAAA,IAAA,KAAQ,IAAA,CAAK,KAAA,CAAM;AACrC,CAAA;ACsoBF,IAAM,gCAAA,GAAmCA,EACtC,MAAA,CAAO;EACN,aAAA,EAAeA,CAAAA,CAAE,MAAA,EAAO,CAAE,OAAA,EAAQ;EAClC,iBAAA,EAAmBA,CAAAA,CAAE,MAAA,EAAO,CAAE,OAAA,EAAQ;EACtC,YAAA,EAAcA,CAAAA,CAAE,MAAA,EAAO,CAAE,OAAA,EAAQ;AACjC,EAAA,qBAAA,EAAuBA,EACpB,MAAA,CAAO;IACN,aAAA,EAAeA,CAAAA,CAAE,MAAA,EAAO,CAAE,OAAA;AAC5B,GAAC,EACA,OAAA,EAAQ;AACX,EAAA,yBAAA,EAA2BA,EACxB,MAAA,CAAO;IACN,gBAAA,EAAkBA,CAAAA,CAAE,MAAA,EAAO,CAAE,OAAA,EAAQ;IACrC,0BAAA,EAA4BA,CAAAA,CAAE,MAAA,EAAO,CAAE,OAAA,EAAQ;IAC/C,0BAAA,EAA4BA,CAAAA,CAAE,MAAA,EAAO,CAAE,OAAA;AACzC,GAAC,EACA,OAAA;AACL,CAAC,EACA,OAAA,EAAQ;AAIgCA,EAAE,MAAA,CAAO;EAClD,EAAA,EAAIA,CAAAA,CAAE,MAAA,EAAO,CAAE,OAAA,EAAQ;EACvB,OAAA,EAASA,CAAAA,CAAE,MAAA,EAAO,CAAE,OAAA,EAAQ;EAC5B,KAAA,EAAOA,CAAAA,CAAE,MAAA,EAAO,CAAE,OAAA,EAAQ;AAC1B,EAAA,OAAA,EAASA,CAAAA,CAAE,KAAA;AACTA,IAAAA,CAAAA,CAAE,MAAA,CAAO;AACP,MAAA,OAAA,EAASA,EAAE,MAAA,CAAO;AAChB,QAAA,IAAA,EAAMA,CAAAA,CAAE,OAAA,CAAQ,WAAW,CAAA,CAAE,OAAA,EAAQ;QACrC,OAAA,EAASA,CAAAA,CAAE,MAAA,EAAO,CAAE,OAAA,EAAQ;QAC5B,iBAAA,EAAmBA,CAAAA,CAAE,MAAA,EAAO,CAAE,OAAA,EAAQ;QACtC,SAAA,EAAWA,CAAAA,CAAE,MAAA,EAAO,CAAE,OAAA,EAAQ;AAC9B,QAAA,UAAA,EAAYA,CAAAA,CACT,KAAA;AACCA,UAAAA,CAAAA,CAAE,MAAA,CAAO;YACP,EAAA,EAAIA,CAAAA,CAAE,MAAA,EAAO,CAAE,OAAA,EAAQ;AACvB,YAAA,QAAA,EAAUA,EAAE,MAAA,CAAO;AACjB,cAAA,IAAA,EAAMA,EAAE,MAAA,EAAO;AACf,cAAA,SAAA,EAAWA,EAAE,MAAA;aACd;WACF;AACH,SAAA,CACC,OAAA;OACJ,CAAA;MACD,aAAA,EAAeA,CAAAA,CAAE,MAAA,EAAO,CAAE,OAAA;KAC3B;AACH,GAAA;EACA,KAAA,EAAO;AACT,CAAC;AChtBwDA,EAAE,MAAA,CAAO;;;;EAIhE,IAAA,EAAMA,CAAAA,CAAE,OAAA,EAAQ,CAAE,QAAA,EAAS;;;;;;;EAQ3B,SAAA,EAAWA,CAAAA,CAAE,OAAOA,CAAAA,CAAE,MAAA,IAAUA,CAAAA,CAAE,MAAA,EAAQ,CAAA,CAAE,QAAA,EAAS;;;;EAKrD,MAAA,EAAQA,CAAAA,CAAE,MAAA,EAAO,CAAE,QAAA,EAAS;;;;;EAM5B,IAAA,EAAMA,CAAAA,CAAE,MAAA,EAAO,CAAE,QAAA;AACnB,CAAC;ACwTD,IAAM,WAAA,GAAcA,EAAE,MAAA,CAAO;AAC3B,EAAA,aAAA,EAAeA,EAAE,MAAA,EAAO;AACxB,EAAA,iBAAA,EAAmBA,EAAE,MAAA,EAAO;AAC5B,EAAA,YAAA,EAAcA,EAAE,MAAA;AAClB,CAAC,CAAA;AAIgDA,EAAE,MAAA,CAAO;EACxD,EAAA,EAAIA,CAAAA,CAAE,MAAA,EAAO,CAAE,OAAA,EAAQ;EACvB,OAAA,EAASA,CAAAA,CAAE,MAAA,EAAO,CAAE,OAAA,EAAQ;EAC5B,KAAA,EAAOA,CAAAA,CAAE,MAAA,EAAO,CAAE,OAAA,EAAQ;AAC1B,EAAA,OAAA,EAASA,CAAAA,CAAE,KAAA;AACTA,IAAAA,CAAAA,CAAE,MAAA,CAAO;AACP,MAAA,IAAA,EAAMA,EAAE,MAAA,EAAO;AACf,MAAA,aAAA,EAAeA,EAAE,MAAA;KAClB;AACH,GAAA;AACA,EAAA,KAAA,EAAO,YAAY,OAAA;AACrB,CAAC;ACnWuDA,EAAE,MAAA,CAAO;;;;;EAK/D,UAAA,EAAYA,CAAAA,CAAE,MAAA,EAAO,CAAE,QAAA,EAAS;;;;;EAMhC,IAAA,EAAMA,CAAAA,CAAE,MAAA,EAAO,CAAE,QAAA;AACnB,CAAC;AC8HyCA,EAAE,MAAA,CAAO;AACjD,EAAA,IAAA,EAAMA,CAAAA,CAAE,KAAA,CAAMA,CAAAA,CAAE,MAAA,CAAO,EAAE,SAAA,EAAWA,CAAAA,CAAE,KAAA,CAAMA,CAAAA,CAAE,MAAA,EAAQ,CAAA,EAAG,CAAC,CAAA;EAC1D,KAAA,EAAOA,CAAAA,CAAE,OAAO,EAAE,aAAA,EAAeA,EAAE,MAAA,EAAO,EAAG,CAAA,CAAE,OAAA,EAAQ;AACvD,EAAA,gBAAA,EAAkBA,CAAAA,CACf,MAAA,CAAOA,CAAAA,CAAE,MAAA,IAAUA,CAAAA,CAAE,MAAA,CAAOA,CAAAA,CAAE,MAAA,IAAUA,CAAAA,CAAE,GAAA,EAAK,CAAC,EAChD,QAAA;AACL,CAAC;AC1HM,IAAM,6BAAN,MAAyD;AAQ9D,EAAA,WAAA,CACW,SACQ,MAAA,EACjB;AAFS,IAAA,IAAA,CAAA,OAAA,GAAA,OAAA;AACQ,IAAA,IAAA,CAAA,MAAA,GAAA,MAAA;AATnB,IAAA,IAAA,CAAS,oBAAA,GAAuB,IAAA;AAChC,IAAA,IAAA,CAAS,gBAAA,GAAmB,EAAA;AASzB,EAAA;AAPH,EAAA,IAAI,QAAA,GAAmB;AACrB,IAAA,OAAO,KAAK,MAAA,CAAO,QAAA;AACrB,EAAA;AAOA,EAAA,MAAM,UAAA,CAAW;AACf,IAAA,MAAA;AACA,IAAA,CAAA;AACA,IAAA,IAAA;AACA,IAAA,WAAA;AACA,IAAA,IAAA;AACA,IAAA,eAAA;AACA,IAAA,OAAA;AACA,IAAA;GACF,EAEE;AAlDJ,IAAA,IAAA,EAAA,EAAA,EAAA,EAAA,EAAA,EAAA,EAAA,EAAA,EAAA;AAmDI,IAAA,MAAM,WAA2C,EAAC;AAElD,IAAA,IAAI,eAAe,IAAA,EAAM;AACvB,MAAA,QAAA,CAAS,IAAA,CAAK;QACZ,IAAA,EAAM,qBAAA;QACN,OAAA,EAAS,aAAA;QACT,OAAA,EACE;OACH,CAAA;AACH,IAAA;AAEA,IAAA,IAAI,QAAQ,IAAA,EAAM;AAChB,MAAA,QAAA,CAAS,KAAK,EAAE,IAAA,EAAM,qBAAA,EAAuB,OAAA,EAAS,QAAQ,CAAA;AAChE,IAAA;AAEA,IAAA,MAAM,WAAA,GAAA,CAAc,MAAA,EAAA,GAAA,CAAA,EAAA,GAAA,KAAK,MAAA,CAAO,SAAA,KAAZ,OAAA,MAAA,GAAA,EAAA,CAAuB,gBAAvB,IAAA,GAAA,MAAA,GAAA,GAAA,IAAA,CAAA,EAAA,MAAA,IAAA,GAAA,EAAA,uBAA8C,IAAA,EAAK;AACvE,IAAA,MAAM,EAAE,KAAA,EAAO,QAAA,EAAU,eAAA,EAAgB,GAAI,MAAMC,aAAAA,CAAc;MAC/D,GAAA,EAAK,IAAA,CAAK,OAAO,GAAA,CAAI;QACnB,IAAA,EAAM,qBAAA;AACN,QAAA,OAAA,EAAS,IAAA,CAAK;OACf,CAAA;AACD,MAAA,OAAA,EAASC,cAAAA,CAAe,IAAA,CAAK,MAAA,CAAO,OAAA,IAAW,OAAO,CAAA;MACtD,IAAA,EAAM;AACJ,QAAA,KAAA,EAAO,IAAA,CAAK,OAAA;AACZ,QAAA,MAAA;AACA,QAAA,CAAA;AACA,QAAA,IAAA;AACA,QAAA,GAAA,CAAI,EAAA,GAAA,eAAA,CAAgB,MAAA,KAAhB,IAAA,GAAA,KAA0B,EAAC;QAC/B,eAAA,EAAiB;AACnB,OAAA;MACA,qBAAA,EAAuBC,8BAAAA;AACrB,QAAA,CAAA,EAAA,GAAA,IAAA,CAAK,MAAA,CAAO,cAAA,KAAZ,IAAA,GAAA,EAAA,GAA8B;AAChC,OAAA;MACA,yBAAA,EAA2BC,yBAAAA;AACzB,QAAA;AACF,OAAA;AACA,MAAA,WAAA;AACA,MAAA,KAAA,EAAO,KAAK,MAAA,CAAO;KACpB,CAAA;AAED,IAAA,OAAO;AACL,MAAA,MAAA,EAAQ,SAAS,IAAA,CAAK,GAAA,CAAI,CAAA,IAAA,KAAQ,KAAK,QAAQ,CAAA;AAC/C,MAAA,QAAA;MACA,QAAA,EAAU;QACR,SAAA,EAAW,WAAA;AACX,QAAA,OAAA,EAAS,IAAA,CAAK,OAAA;QACd,OAAA,EAAS;AACX;AACF,KAAA;AACF,EAAA;AACF,CAAA;AAIA,IAAM,mCAAA,GAAsCJ,EAAE,MAAA,CAAO;EACnD,IAAA,EAAMA,CAAAA,CAAE,KAAA,CAAMA,CAAAA,CAAE,MAAA,CAAO,EAAE,UAAUA,CAAAA,CAAE,MAAA,EAAO,EAAG,CAAC;AAClD,CAAC,CAAA;ACnGM,SAAS,yBAAyB,MAAA,EAGvC;AACA,EAAA,MAAM,WAA0B,EAAC;AACjC,EAAA,MAAM,WAA8C,EAAC;AAErD,EAAA,KAAA,MAAW,EAAE,IAAA,EAAM,OAAA,EAAQ,IAAK,MAAA,EAAQ;AACtC,IAAA,QAAQ,IAAA;AACN,MAAA,KAAK,QAAA,EAAU;AACb,QAAA,QAAA,CAAS,IAAA,CAAK,EAAE,IAAA,EAAM,QAAA,EAAU,SAAS,CAAA;AACzC,QAAA;AACF,MAAA;AAEA,MAAA,KAAK,MAAA,EAAQ;AACX,QAAA,IAAI,QAAQ,MAAA,KAAW,CAAA,IAAK,QAAQ,CAAC,CAAA,CAAE,SAAS,MAAA,EAAQ;AACtD,UAAA,QAAA,CAAS,IAAA,CAAK,EAAE,IAAA,EAAM,MAAA,EAAQ,SAAS,OAAA,CAAQ,CAAC,CAAA,CAAE,IAAA,EAAM,CAAA;AACxD,UAAA;AACF,QAAA;AAEA,QAAA,QAAA,CAAS,IAAA,CAAK;UACZ,IAAA,EAAM,MAAA;UACN,OAAA,EAAS,OAAA,CAAQ,GAAA,CAAI,CAAA,IAAA,KAAQ;AAC3B,YAAA,QAAQ,KAAK,IAAA;AACX,cAAA,KAAK,MAAA,EAAQ;AACX,gBAAA,OAAO,EAAE,IAAA,EAAM,MAAA,EAAQ,IAAA,EAAM,KAAK,IAAA,EAAK;AACzC,cAAA;AACA,cAAA,KAAK,MAAA,EAAQ;AACX,gBAAA,IAAI,IAAA,CAAK,SAAA,CAAU,UAAA,CAAW,QAAQ,CAAA,EAAG;AACvC,kBAAA,MAAM,SAAA,GACJ,IAAA,CAAK,SAAA,KAAc,SAAA,GACf,eACA,IAAA,CAAK,SAAA;AAEX,kBAAA,OAAO;oBACL,IAAA,EAAM,WAAA;oBACN,SAAA,EAAW;AACT,sBAAA,GAAA,EACE,IAAA,CAAK,IAAA,YAAgB,GAAA,GACjB,IAAA,CAAK,IAAA,CAAK,QAAA,EAAS,GACnB,CAAA,KAAA,EAAQ,SAAS,CAAA,QAAA,EAAW,eAAA,CAAgB,IAAA,CAAK,IAAI,CAAC,CAAA;AAC9D;AACF,mBAAA;gBACF,CAAA,MAAO;AACL,kBAAA,MAAM,IAAI,6BAAA,CAA8B;oBACtC,aAAA,EAAe,CAAA,qBAAA,EAAwB,KAAK,SAAS,CAAA;mBACtD,CAAA;AACH,gBAAA;AACF,cAAA;AACF;UACF,CAAC;SACF,CAAA;AAED,QAAA;AACF,MAAA;AAEA,MAAA,KAAK,WAAA,EAAa;AAChB,QAAA,IAAI,IAAA,GAAO,EAAA;AACX,QAAA,MAAM,YAID,EAAC;AAEN,QAAA,KAAA,MAAW,QAAQ,OAAA,EAAS;AAC1B,UAAA,QAAQ,KAAK,IAAA;AACX,YAAA,KAAK,MAAA,EAAQ;AACX,cAAA,IAAA,IAAQ,IAAA,CAAK,IAAA;AACb,cAAA;AACF,YAAA;AACA,YAAA,KAAK,WAAA,EAAa;AAChB,cAAA,SAAA,CAAU,IAAA,CAAK;AACb,gBAAA,EAAA,EAAI,IAAA,CAAK,UAAA;gBACT,IAAA,EAAM,UAAA;gBACN,QAAA,EAAU;AACR,kBAAA,IAAA,EAAM,IAAA,CAAK,QAAA;kBACX,SAAA,EAAW,IAAA,CAAK,SAAA,CAAU,IAAA,CAAK,KAAK;AACtC;eACD,CAAA;AACD,cAAA;AACF,YAAA;AACF;AACF,QAAA;AAEA,QAAA,QAAA,CAAS,IAAA,CAAK;UACZ,IAAA,EAAM,WAAA;UACN,OAAA,EAAS,IAAA;UACT,UAAA,EAAY,SAAA,CAAU,MAAA,GAAS,CAAA,GAAI,SAAA,GAAY;SAChD,CAAA;AAED,QAAA;AACF,MAAA;AAEA,MAAA,KAAK,MAAA,EAAQ;AACX,QAAA,KAAA,MAAW,gBAAgB,OAAA,EAAS;AAClC,UAAA,MAAM,SAAS,YAAA,CAAa,MAAA;AAE5B,UAAA,IAAI,YAAA;AACJ,UAAA,QAAQ,OAAO,IAAA;YACb,KAAK,MAAA;YACL,KAAK,YAAA;AACH,cAAA,YAAA,GAAe,MAAA,CAAO,KAAA;AACtB,cAAA;YACF,KAAK,SAAA;YACL,KAAK,MAAA;YACL,KAAK,YAAA;AACH,cAAA,YAAA,GAAe,IAAA,CAAK,SAAA,CAAU,MAAA,CAAO,KAAK,CAAA;AAC1C,cAAA;AACJ;AAEA,UAAA,QAAA,CAAS,IAAA,CAAK;YACZ,IAAA,EAAM,MAAA;AACN,YAAA,YAAA,EAAc,YAAA,CAAa,UAAA;YAC3B,OAAA,EAAS;WACV,CAAA;AACH,QAAA;AACA,QAAA;AACF,MAAA;MAEA,SAAS;AACP,QAAA,MAAM,gBAAA,GAA0B,IAAA;AAChC,QAAA,MAAM,IAAI,KAAA,CAAM,CAAA,kBAAA,EAAqB,gBAAgB,CAAA,CAAE,CAAA;AACzD,MAAA;AACF;AACF,EAAA;AAEA,EAAA,OAAO,EAAE,UAAU,QAAA,EAAS;AAC9B;ACvIO,SAAS,mBAAA,CAAoB;AAClC,EAAA,EAAA;AACA,EAAA,KAAA;AACA,EAAA;AACF,CAAA,EAIG;AACD,EAAA,OAAO;IACL,EAAA,EAAI,EAAA,IAAA,OAAA,EAAA,GAAM,MAAA;IACV,OAAA,EAAS,KAAA,IAAA,OAAA,KAAA,GAAS,MAAA;AAClB,IAAA,SAAA,EAAW,WAAW,IAAA,GAAO,IAAI,IAAA,CAAK,OAAA,GAAU,GAAI,CAAA,GAAI;AAC1D,GAAA;AACF;ACZO,SAAS,mBACd,YAAA,EAC6B;AAC7B,EAAA,QAAQ,YAAA;IACN,KAAK,MAAA;AACH,MAAA,OAAO,MAAA;IACT,KAAK,QAAA;AACH,MAAA,OAAO,QAAA;IACT,KAAK,YAAA;IACL,KAAK,eAAA;AACH,MAAA,OAAO,YAAA;IACT,KAAK,gBAAA;AACH,MAAA,OAAO,gBAAA;AACT,IAAA;AACE,MAAA,OAAO,SAAA;AACX;AACF;ACWA,IAAM,eAAA,GAAkBA,EAAE,MAAA,CAAO;EAC/B,IAAA,EAAMA,CAAAA,CAAE,QAAQ,KAAK,CAAA;AACrB,EAAA,OAAA,EAASA,EAAE,MAAA,EAAO,CAAE,MAAA,CAAO,CAAC,EAAE,QAAA,EAAS;EACvC,gBAAA,EAAkBA,CAAAA,CAAE,MAAMA,CAAAA,CAAE,MAAA,EAAQ,CAAA,CAAE,GAAA,CAAI,CAAC,CAAA,CAAE,QAAA,EAAS;EACtD,eAAA,EAAiBA,CAAAA,CAAE,MAAMA,CAAAA,CAAE,MAAA,EAAQ,CAAA,CAAE,GAAA,CAAI,CAAC,CAAA,CAAE,QAAA,EAAS;EACrD,UAAA,EAAYA,CAAAA,CAAE,OAAA,EAAQ,CAAE,QAAA;AAC1B,CAAC,CAAA;AAED,IAAM,aAAA,GAAgBA,EAAE,MAAA,CAAO;EAC7B,IAAA,EAAMA,CAAAA,CAAE,QAAQ,GAAG,CAAA;AACnB,EAAA,QAAA,EAAUA,EAAE,KAAA,CAAMA,CAAAA,CAAE,MAAA,EAAQ,EAAE,QAAA;AAChC,CAAC,CAAA;AAED,IAAM,gBAAA,GAAmBA,EAAE,MAAA,CAAO;EAChC,IAAA,EAAMA,CAAAA,CAAE,QAAQ,MAAM,CAAA;AACtB,EAAA,OAAA,EAASA,EAAE,MAAA,EAAO,CAAE,MAAA,CAAO,CAAC,EAAE,QAAA,EAAS;EACvC,gBAAA,EAAkBA,CAAAA,CAAE,MAAMA,CAAAA,CAAE,MAAA,EAAQ,CAAA,CAAE,GAAA,CAAI,CAAC,CAAA,CAAE,QAAA,EAAS;EACtD,UAAA,EAAYA,CAAAA,CAAE,OAAA,EAAQ,CAAE,QAAA;AAC1B,CAAC,CAAA;AAED,IAAM,eAAA,GAAkBA,EAAE,MAAA,CAAO;EAC/B,IAAA,EAAMA,CAAAA,CAAE,QAAQ,KAAK,CAAA;EACrB,KAAA,EAAOA,CAAAA,CAAE,MAAMA,CAAAA,CAAE,MAAA,GAAS,GAAA,EAAK,CAAA,CAAE,GAAA,CAAI,CAAC;;AACxC,CAAC,CAAA;AAED,IAAM,kBAAA,GAAqBA,CAAAA,CAAE,kBAAA,CAAmB,MAAA,EAAQ;AACtD,EAAA,eAAA;AACA,EAAA,aAAA;AACA,EAAA,gBAAA;AACA,EAAA;AACF,CAAC,CAAA;AAGM,IAAM,kBAAA,GAAqBA,EAAE,MAAA,CAAO;;;;;AAKzC,EAAA,eAAA,EAAiBA,EAAE,IAAA,CAAK,CAAC,OAAO,MAAM,CAAC,EAAE,QAAA,EAAS;AAElD,EAAA,gBAAA,EAAkBA,EACf,MAAA,CAAO;;;;;;;AAON,IAAA,IAAA,EAAMA,EAAE,IAAA,CAAK,CAAC,KAAA,EAAO,MAAA,EAAQ,IAAI,CAAC,CAAA;;;;;IAMlC,eAAA,EAAiBA,CAAAA,CAAE,OAAA,EAAQ,CAAE,QAAA,EAAS;;;;IAKtC,QAAA,EAAUA,CAAAA,CAAE,MAAA,EAAO,CAAE,QAAA,EAAS;;;;IAK9B,MAAA,EAAQA,CAAAA,CAAE,MAAA,EAAO,CAAE,QAAA,EAAS;;;;;IAM5B,gBAAA,EAAkBA,CAAAA,CAAE,QAAO,CAAE,GAAA,CAAI,CAAC,CAAA,CAAE,GAAA,CAAI,EAAE,CAAA,CAAE,QAAA,EAAS;;;;;AAMrD,IAAA,OAAA,EAASA,CAAAA,CAAE,KAAA,CAAM,kBAAkB,CAAA,CAAE,QAAA;AACvC,GAAC,EACA,QAAA;AACL,CAAC,CAAA;ACxGM,IAAM,kBAAA,GAAqBA,EAAE,MAAA,CAAO;AACzC,EAAA,KAAA,EAAOA,EAAE,MAAA,CAAO;AACd,IAAA,OAAA,EAASA,EAAE,MAAA,EAAO;IAClB,IAAA,EAAMA,CAAAA,CAAE,MAAA,EAAO,CAAE,OAAA,EAAQ;IACzB,KAAA,EAAOA,CAAAA,CAAE,GAAA,EAAI,CAAE,OAAA,EAAQ;IACvB,IAAA,EAAMA,CAAAA,CAAE,KAAA,CAAM,CAACA,CAAAA,CAAE,MAAA,EAAO,EAAGA,CAAAA,CAAE,MAAA,EAAQ,CAAC,CAAA,CAAE,OAAA;GACzC;AACH,CAAC,CAAA;AAIM,IAAM,2BAA2B,8BAAA,CAA+B;EACrE,WAAA,EAAa,kBAAA;EACb,cAAA,EAAgB,CAAA,IAAA,KAAQ,IAAA,CAAK,KAAA,CAAM;AACrC,CAAC,CAAA;ACXM,SAAS,YAAA,CAAa;AAC3B,EAAA,KAAA;AACA,EAAA;AACF,CAAA,EAgBE;AAEA,EAAA,KAAA,GAAA,CAAQ,KAAA,IAAA,IAAA,GAAA,MAAA,GAAA,KAAA,CAAO,UAAS,KAAA,GAAQ,MAAA;AAEhC,EAAA,MAAM,eAA6C,EAAC;AAEpD,EAAA,IAAI,SAAS,IAAA,EAAM;AACjB,IAAA,OAAO,EAAE,KAAA,EAAO,MAAA,EAAW,UAAA,EAAY,QAAW,YAAA,EAAa;AACjE,EAAA;AAGA,EAAA,MAAM,WAOD,EAAC;AAEN,EAAA,KAAA,MAAW,QAAQ,KAAA,EAAO;AACxB,IAAA,IAAI,IAAA,CAAK,SAAS,kBAAA,EAAoB;AACpC,MAAA,YAAA,CAAa,IAAA,CAAK,EAAE,IAAA,EAAM,kBAAA,EAAoB,MAAM,CAAA;IACtD,CAAA,MAAO;AACL,MAAA,QAAA,CAAS,IAAA,CAAK;QACZ,IAAA,EAAM,UAAA;QACN,QAAA,EAAU;AACR,UAAA,IAAA,EAAM,IAAA,CAAK,IAAA;AACX,UAAA,WAAA,EAAa,IAAA,CAAK,WAAA;AAClB,UAAA,UAAA,EAAY,IAAA,CAAK;AACnB;OACD,CAAA;AACH,IAAA;AACF,EAAA;AAEA,EAAA,IAAI,cAAc,IAAA,EAAM;AACtB,IAAA,OAAO,EAAE,KAAA,EAAO,QAAA,EAAU,UAAA,EAAY,QAAW,YAAA,EAAa;AAChE,EAAA;AAEA,EAAA,MAAM,OAAO,UAAA,CAAW,IAAA;AAExB,EAAA,QAAQ,IAAA;IACN,KAAK,MAAA;IACL,KAAK,MAAA;AACH,MAAA,OAAO,EAAE,KAAA,EAAO,QAAA,EAAU,UAAA,EAAY,MAAM,YAAA,EAAa;IAC3D,KAAK,UAAA;AAEH,MAAA,OAAO,EAAE,KAAA,EAAO,QAAA,EAAU,UAAA,EAAY,YAAY,YAAA,EAAa;IACjE,KAAK,MAAA;AAEH,MAAA,OAAO;QACL,KAAA,EAAO,QAAA;QACP,UAAA,EAAY;UACV,IAAA,EAAM,UAAA;UACN,QAAA,EAAU,EAAE,IAAA,EAAM,UAAA,CAAW,QAAA;AAC/B,SAAA;AACA,QAAA;AACF,OAAA;IACF,SAAS;AACP,MAAA,MAAM,gBAAA,GAA0B,IAAA;AAChC,MAAA,MAAM,IAAIK,6BAAAA,CAA8B;AACtC,QAAA,aAAA,EAAe,qBAAqB,gBAAgB,CAAA;OACrD,CAAA;AACH,IAAA;AACF;AACF;AC1DO,IAAM,uBAAN,MAAsD;AAO3D,EAAA,WAAA,CAAY,SAAyB,MAAA,EAAuB;AAN5D,IAAA,IAAA,CAAS,oBAAA,GAAuB,IAAA;AAehC,IAAA,IAAA,CAAS,aAAA,GAA0C;AACjD,MAAA,SAAA,EAAW,CAAC,iBAAiB;AAC/B,KAAA;AAVE,IAAA,IAAA,CAAK,OAAA,GAAU,OAAA;AACf,IAAA,IAAA,CAAK,MAAA,GAAS,MAAA;AAChB,EAAA;AAEA,EAAA,IAAI,QAAA,GAAmB;AACrB,IAAA,OAAO,KAAK,MAAA,CAAO,QAAA;AACrB,EAAA;AAMA,EAAA,MAAc,OAAA,CAAQ;AACpB,IAAA,MAAA;AACA,IAAA,eAAA;AACA,IAAA,WAAA;AACA,IAAA,IAAA;AACA,IAAA,IAAA;AACA,IAAA,gBAAA;AACA,IAAA,eAAA;AACA,IAAA,aAAA;AACA,IAAA,IAAA;AACA,IAAA,cAAA;AACA,IAAA,eAAA;AACA,IAAA,KAAA;AACA,IAAA;GACF,EAAiD;AAnEnD,IAAA,IAAA,IAAA,EAAA,EAAA,EAAA;AAoEI,IAAA,MAAM,WAAyC,EAAC;AAGhD,IAAA,MAAM,OAAA,GAAA,CACH,EAAA,GAAA,MAAM,oBAAA,CAAqB;MAC1B,QAAA,EAAU,KAAA;AACV,MAAA,eAAA;MACA,MAAA,EAAQ;KACT,CAAA,KAJA,IAAA,GAAA,EAAA,GAIM,EAAC;AAGV,IAAA,IAAI,QAAQ,IAAA,EAAM;AAChB,MAAA,QAAA,CAAS,IAAA,CAAK;QACZ,IAAA,EAAM,qBAAA;QACN,OAAA,EAAS;OACV,CAAA;AACH,IAAA;AAEA,IAAA,IAAI,oBAAoB,IAAA,EAAM;AAC5B,MAAA,QAAA,CAAS,IAAA,CAAK;QACZ,IAAA,EAAM,qBAAA;QACN,OAAA,EAAS;OACV,CAAA;AACH,IAAA;AAEA,IAAA,IAAI,mBAAmB,IAAA,EAAM;AAC3B,MAAA,QAAA,CAAS,IAAA,CAAK;QACZ,IAAA,EAAM,qBAAA;QACN,OAAA,EAAS;OACV,CAAA;AACH,IAAA;AAEA,IAAA,IAAI,iBAAiB,IAAA,EAAM;AACzB,MAAA,QAAA,CAAS,IAAA,CAAK;QACZ,IAAA,EAAM,qBAAA;QACN,OAAA,EAAS;OACV,CAAA;AACH,IAAA;AAEA,IAAA,IACE,kBAAkB,IAAA,IAClB,cAAA,CAAe,SAAS,MAAA,IACxB,cAAA,CAAe,UAAU,IAAA,EACzB;AACA,MAAA,QAAA,CAAS,IAAA,CAAK;QACZ,IAAA,EAAM,qBAAA;QACN,OAAA,EAAS,gBAAA;QACT,OAAA,EAAS;OACV,CAAA;AACH,IAAA;AAGA,IAAA,MAAM,EAAE,QAAA,EAAU,QAAA,EAAU,eAAA,EAAgB,GAC1C,yBAAyB,MAAM,CAAA;AACjC,IAAA,QAAA,CAAS,IAAA,CAAK,GAAG,eAAe,CAAA;AAGhC,IAAA,MAAM;MACJ,KAAA,EAAO,QAAA;MACP,UAAA,EAAY,aAAA;AACZ,MAAA;AACF,KAAA,GAAI,YAAA,CAAa;AACf,MAAA,KAAA;AACA,MAAA;KACD,CAAA;AACD,IAAA,QAAA,CAAS,IAAA,CAAK,GAAG,YAAY,CAAA;AAE7B,IAAA,MAAM,QAAA,GAAW;;AAEf,MAAA,KAAA,EAAO,IAAA,CAAK,OAAA;;MAGZ,UAAA,EAAY,eAAA;AACZ,MAAA,WAAA;MACA,KAAA,EAAO,IAAA;AACP,MAAA,IAAA;AACA,MAAA,gBAAA,EAAkB,OAAA,CAAQ,eAAA;;MAG1B,eAAA,EAAA,CACE,cAAA,IAAA,OAAA,MAAA,GAAA,cAAA,CAAgB,UAAS,MAAA,GACrB,cAAA,CAAe,UAAU,IAAA,GACvB;QACE,IAAA,EAAM,aAAA;QACN,WAAA,EAAa;AACX,UAAA,IAAA,EAAA,CAAM,EAAA,GAAA,cAAA,CAAe,IAAA,KAAf,IAAA,GAAA,EAAA,GAAuB,UAAA;AAC7B,UAAA,MAAA,EAAQ,cAAA,CAAe,MAAA;UACvB,MAAA,EAAQ;AACV;OACF,GACA,EAAE,IAAA,EAAM,aAAA,EAAc,GACxB,MAAA;;AAGN,MAAA,iBAAA,EAAmB,QAAQ,gBAAA,GACvB;AACE,QAAA,IAAA,EAAM,QAAQ,gBAAA,CAAiB,IAAA;AAC/B,QAAA,gBAAA,EAAkB,QAAQ,gBAAA,CAAiB,eAAA;AAC3C,QAAA,SAAA,EAAW,QAAQ,gBAAA,CAAiB,QAAA;AACpC,QAAA,OAAA,EAAS,QAAQ,gBAAA,CAAiB,MAAA;AAClC,QAAA,kBAAA,EAAoB,QAAQ,gBAAA,CAAiB,gBAAA;QAC7C,OAAA,EAAA,CAAS,EAAA,GAAA,QAAQ,gBAAA,CAAiB,OAAA,KAAzB,OAAA,MAAA,GAAA,EAAA,CAAkC,GAAA,CAAI,CAAA,MAAA,MAAW;AACxD,UAAA,IAAA,EAAM,MAAA,CAAO,IAAA;UACb,GAAI,MAAA,CAAO,SAAS,KAAA,IAAS;AAC3B,YAAA,OAAA,EAAS,MAAA,CAAO,OAAA;AAChB,YAAA,iBAAA,EAAmB,MAAA,CAAO,gBAAA;AAC1B,YAAA,gBAAA,EAAkB,MAAA,CAAO,eAAA;AACzB,YAAA,WAAA,EAAa,MAAA,CAAO;AACtB,WAAA;UACA,GAAI,MAAA,CAAO,SAAS,GAAA,IAAO;AACzB,YAAA,SAAA,EAAW,MAAA,CAAO;AACpB,WAAA;UACA,GAAI,MAAA,CAAO,SAAS,MAAA,IAAU;AAC5B,YAAA,OAAA,EAAS,MAAA,CAAO,OAAA;AAChB,YAAA,iBAAA,EAAmB,MAAA,CAAO,gBAAA;AAC1B,YAAA,WAAA,EAAa,MAAA,CAAO;AACtB,WAAA;UACA,GAAI,MAAA,CAAO,SAAS,KAAA,IAAS;AAC3B,YAAA,KAAA,EAAO,MAAA,CAAO;AAChB;SACF,CAAA;OACF,GACA,MAAA;;AAGJ,MAAA,QAAA;;MAGA,KAAA,EAAO,QAAA;MACP,WAAA,EAAa;AACf,KAAA;AAEA,IAAA,OAAO;MACL,IAAA,EAAM,QAAA;AACN,MAAA;AACF,KAAA;AACF,EAAA;AAEA,EAAA,MAAM,WACJ,OAAA,EAC6D;AAhNjE,IAAA,IAAA,IAAA,EAAA,EAAA,EAAA;AAiNI,IAAA,MAAM,EAAE,MAAM,IAAA,EAAM,QAAA,KAAa,MAAM,IAAA,CAAK,QAAQ,OAAO,CAAA;AAE3D,IAAA,MAAM;AACJ,MAAA,eAAA;MACA,KAAA,EAAO,QAAA;MACP,QAAA,EAAU;AACZ,KAAA,GAAI,MAAM,aAAA,CAAc;AACtB,MAAA,GAAA,EAAK,IAAG,EAAA,GAAA,IAAA,CAAK,OAAO,OAAA,KAAZ,IAAA,GAAA,KAAuB,qBAAqB,CAAA,iBAAA,CAAA;AACpD,MAAA,OAAA,EAAS,eAAe,IAAA,CAAK,MAAA,CAAO,OAAA,EAAQ,EAAG,QAAQ,OAAO,CAAA;AAC9D,MAAA,IAAA;MACA,qBAAA,EAAuB,wBAAA;MACvB,yBAAA,EAA2B,yBAAA;AACzB,QAAA;AACF,OAAA;AACA,MAAA,WAAA,EAAa,OAAA,CAAQ,WAAA;AACrB,MAAA,KAAA,EAAO,KAAK,MAAA,CAAO;KACpB,CAAA;AAED,IAAA,MAAM,MAAA,GAAS,QAAA,CAAS,OAAA,CAAQ,CAAC,CAAA;AACjC,IAAA,MAAM,UAAyC,EAAC;AAGhD,IAAA,IAAI,MAAA,CAAO,QAAQ,OAAA,IAAW,IAAA,IAAQ,OAAO,OAAA,CAAQ,OAAA,CAAQ,SAAS,CAAA,EAAG;AACvE,MAAA,IAAI,IAAA,GAAO,OAAO,OAAA,CAAQ,OAAA;AAG1B,MAAA,MAAM,cAAc,IAAA,CAAK,QAAA,CAAS,IAAA,CAAK,QAAA,CAAS,SAAS,CAAC,CAAA;AAC1D,MAAA,IAAA,CAAI,WAAA,IAAA,OAAA,MAAA,GAAA,WAAA,CAAa,UAAS,WAAA,IAAe,IAAA,KAAS,YAAY,OAAA,EAAS;AACrE,QAAA,IAAA,GAAO,EAAA;AACT,MAAA;AAEA,MAAA,IAAI,IAAA,CAAK,SAAS,CAAA,EAAG;AACnB,QAAA,OAAA,CAAQ,IAAA,CAAK,EAAE,IAAA,EAAM,MAAA,EAAQ,MAAM,CAAA;AACrC,MAAA;AACF,IAAA;AAGA,IAAA,IACE,MAAA,CAAO,QAAQ,iBAAA,IAAqB,IAAA,IACpC,OAAO,OAAA,CAAQ,iBAAA,CAAkB,SAAS,CAAA,EAC1C;AACA,MAAA,OAAA,CAAQ,IAAA,CAAK;QACX,IAAA,EAAM,WAAA;AACN,QAAA,IAAA,EAAM,OAAO,OAAA,CAAQ;OACtB,CAAA;AACH,IAAA;AAGA,IAAA,IAAI,MAAA,CAAO,OAAA,CAAQ,UAAA,IAAc,IAAA,EAAM;AACrC,MAAA,KAAA,MAAW,QAAA,IAAY,MAAA,CAAO,OAAA,CAAQ,UAAA,EAAY;AAChD,QAAA,OAAA,CAAQ,IAAA,CAAK;UACX,IAAA,EAAM,WAAA;AACN,UAAA,UAAA,EAAY,QAAA,CAAS,EAAA;AACrB,UAAA,QAAA,EAAU,SAAS,QAAA,CAAS,IAAA;AAC5B,UAAA,KAAA,EAAO,SAAS,QAAA,CAAS;SAC1B,CAAA;AACH,MAAA;AACF,IAAA;AAGA,IAAA,IAAI,QAAA,CAAS,aAAa,IAAA,EAAM;AAC9B,MAAA,KAAA,MAAW,GAAA,IAAO,SAAS,SAAA,EAAW;AACpC,QAAA,OAAA,CAAQ,IAAA,CAAK;UACX,IAAA,EAAM,QAAA;UACN,UAAA,EAAY,KAAA;UACZ,EAAA,EAAI,IAAA,CAAK,OAAO,UAAA,EAAW;AAC3B,UAAA;SACD,CAAA;AACH,MAAA;AACF,IAAA;AAEA,IAAA,OAAO;AACL,MAAA,OAAA;MACA,YAAA,EAAc,kBAAA,CAAmB,OAAO,aAAa,CAAA;MACrD,KAAA,EAAO;AACL,QAAA,WAAA,EAAa,SAAS,KAAA,CAAM,aAAA;AAC5B,QAAA,YAAA,EAAc,SAAS,KAAA,CAAM,iBAAA;AAC7B,QAAA,WAAA,EAAa,SAAS,KAAA,CAAM,YAAA;QAC5B,eAAA,EAAA,CACE,EAAA,GAAA,CAAA,EAAA,GAAA,QAAA,CAAS,KAAA,CAAM,yBAAA,KAAf,OAAA,MAAA,GAAA,EAAA,CAA0C,gBAAA,KAA1C,IAAA,GAAA,EAAA,GACA;AACJ,OAAA;AACA,MAAA,OAAA,EAAS,EAAE,IAAA,EAAK;MAChB,QAAA,EAAU;AACR,QAAA,GAAG,oBAAoB,QAAQ,CAAA;QAC/B,OAAA,EAAS,eAAA;QACT,IAAA,EAAM;AACR,OAAA;AACA,MAAA;AACF,KAAA;AACF,EAAA;AAEA,EAAA,MAAM,SACJ,OAAA,EAC2D;AA/S/D,IAAA,IAAA,EAAA;AAgTI,IAAA,MAAM,EAAE,IAAA,EAAM,QAAA,KAAa,MAAM,IAAA,CAAK,QAAQ,OAAO,CAAA;AACrD,IAAA,MAAM,IAAA,GAAO;MACX,GAAG,IAAA;MACH,MAAA,EAAQ,IAAA;MACR,cAAA,EAAgB;QACd,aAAA,EAAe;AACjB;AACF,KAAA;AAEA,IAAA,MAAM,EAAE,eAAA,EAAiB,KAAA,EAAO,QAAA,EAAS,GAAI,MAAM,aAAA,CAAc;AAC/D,MAAA,GAAA,EAAK,IAAG,EAAA,GAAA,IAAA,CAAK,OAAO,OAAA,KAAZ,IAAA,GAAA,KAAuB,qBAAqB,CAAA,iBAAA,CAAA;AACpD,MAAA,OAAA,EAAS,eAAe,IAAA,CAAK,MAAA,CAAO,OAAA,EAAQ,EAAG,QAAQ,OAAO,CAAA;AAC9D,MAAA,IAAA;MACA,qBAAA,EAAuB,wBAAA;AACvB,MAAA,yBAAA,EACE,iCAAiC,kBAAkB,CAAA;AACrD,MAAA,WAAA,EAAa,OAAA,CAAQ,WAAA;AACrB,MAAA,KAAA,EAAO,KAAK,MAAA,CAAO;KACpB,CAAA;AAED,IAAA,IAAI,YAAA,GAA4C,SAAA;AAChD,IAAA,MAAM,KAAA,GAA8B;MAClC,WAAA,EAAa,MAAA;MACb,YAAA,EAAc,MAAA;MACd,WAAA,EAAa;AACf,KAAA;AACA,IAAA,IAAI,YAAA,GAAe,IAAA;AACnB,IAAA,MAAM,gBAAgE,EAAC;AACvE,IAAA,MAAM,sBAA8C,EAAC;AAErD,IAAA,MAAM,IAAA,GAAO,IAAA;AAEb,IAAA,OAAO;AACL,MAAA,MAAA,EAAQ,QAAA,CAAS,WAAA;AACf,QAAA,IAAI,eAAA,CAGF;AACA,UAAA,KAAA,CAAM,UAAA,EAAY;AAChB,YAAA,UAAA,CAAW,OAAA,CAAQ,EAAE,IAAA,EAAM,cAAA,EAAgB,UAAU,CAAA;AACvD,UAAA,CAAA;AAEA,UAAA,SAAA,CAAU,OAAO,UAAA,EAAY;AA1VvC,YAAA,IAAAC,GAAAA,EAAA,EAAA;AA4VY,YAAA,IAAI,QAAQ,gBAAA,EAAkB;AAC5B,cAAA,UAAA,CAAW,QAAQ,EAAE,IAAA,EAAM,OAAO,QAAA,EAAU,KAAA,CAAM,UAAU,CAAA;AAC9D,YAAA;AAEA,YAAA,IAAI,CAAC,MAAM,OAAA,EAAS;AAClB,cAAA,UAAA,CAAW,QAAQ,EAAE,IAAA,EAAM,SAAS,KAAA,EAAO,KAAA,CAAM,OAAO,CAAA;AACxD,cAAA;AACF,YAAA;AAEA,YAAA,MAAM,QAAQ,KAAA,CAAM,KAAA;AAGpB,YAAA,IAAI,YAAA,EAAc;AAChB,cAAA,UAAA,CAAW,OAAA,CAAQ;gBACjB,IAAA,EAAM,mBAAA;AACN,gBAAA,GAAG,oBAAoB,KAAK;eAC7B,CAAA;AACD,cAAA,YAAA,GAAe,KAAA;AACjB,YAAA;AAGA,YAAA,IAAI,KAAA,CAAM,aAAa,IAAA,EAAM;AAC3B,cAAA,KAAA,MAAW,GAAA,IAAO,MAAM,SAAA,EAAW;AACjC,gBAAA,UAAA,CAAW,OAAA,CAAQ;kBACjB,IAAA,EAAM,QAAA;kBACN,UAAA,EAAY,KAAA;kBACZ,EAAA,EAAI,IAAA,CAAK,OAAO,UAAA,EAAW;AAC3B,kBAAA;iBACD,CAAA;AACH,cAAA;AACF,YAAA;AAGA,YAAA,IAAI,KAAA,CAAM,SAAS,IAAA,EAAM;AACvB,cAAA,KAAA,CAAM,WAAA,GAAc,MAAM,KAAA,CAAM,aAAA;AAChC,cAAA,KAAA,CAAM,YAAA,GAAe,MAAM,KAAA,CAAM,iBAAA;AACjC,cAAA,KAAA,CAAM,WAAA,GAAc,MAAM,KAAA,CAAM,YAAA;AAChC,cAAA,KAAA,CAAM,eAAA,GAAA,CACJ,EAAA,GAAA,CAAAA,GAAAA,GAAA,KAAA,CAAM,KAAA,CAAM,yBAAA,KAAZ,IAAA,GAAA,MAAA,GAAAA,GAAAA,CAAuC,gBAAA,KAAvC,IAAA,GAAA,EAAA,GACA,MAAA;AACJ,YAAA;AAEA,YAAA,MAAM,MAAA,GAAS,KAAA,CAAM,OAAA,CAAQ,CAAC,CAAA;AAG9B,YAAA,IAAA,CAAI,MAAA,IAAA,IAAA,GAAA,MAAA,GAAA,MAAA,CAAQ,kBAAiB,IAAA,EAAM;AACjC,cAAA,YAAA,GAAe,kBAAA,CAAmB,OAAO,aAAa,CAAA;AACxD,YAAA;AAGA,YAAA,IAAA,CAAI,MAAA,IAAA,IAAA,GAAA,MAAA,GAAA,MAAA,CAAQ,UAAS,IAAA,EAAM;AACzB,cAAA;AACF,YAAA;AAEA,YAAA,MAAM,QAAQ,MAAA,CAAO,KAAA;AACrB,YAAA,MAAM,cAAc,MAAA,CAAO,KAAA;AAG3B,YAAA,IAAI,MAAM,OAAA,IAAW,IAAA,IAAQ,KAAA,CAAM,OAAA,CAAQ,SAAS,CAAA,EAAG;AACrD,cAAA,MAAM,cAAc,KAAA,CAAM,OAAA;AAG1B,cAAA,MAAM,cAAc,IAAA,CAAK,QAAA,CAAS,IAAA,CAAK,QAAA,CAAS,SAAS,CAAC,CAAA;AAC1D,cAAA,IAAA,CACE,WAAA,IAAA,OAAA,MAAA,GAAA,WAAA,CAAa,UAAS,WAAA,IACtB,WAAA,KAAgB,YAAY,OAAA,EAC5B;AACA,gBAAA;AACF,cAAA;AAEA,cAAA,MAAM,OAAA,GAAU,CAAA,KAAA,EAAQ,KAAA,CAAM,EAAA,IAAM,WAAW,CAAA,CAAA;AAE/C,cAAA,IAAI,aAAA,CAAc,OAAO,CAAA,IAAK,IAAA,EAAM;AAClC,gBAAA,aAAA,CAAc,OAAO,CAAA,GAAI,EAAE,IAAA,EAAM,MAAA,EAAO;AACxC,gBAAA,UAAA,CAAW,OAAA,CAAQ;kBACjB,IAAA,EAAM,YAAA;kBACN,EAAA,EAAI;iBACL,CAAA;AACH,cAAA;AAEA,cAAA,UAAA,CAAW,OAAA,CAAQ;gBACjB,IAAA,EAAM,YAAA;gBACN,EAAA,EAAI,OAAA;gBACJ,KAAA,EAAO;eACR,CAAA;AACH,YAAA;AAGA,YAAA,IACE,MAAM,iBAAA,IAAqB,IAAA,IAC3B,KAAA,CAAM,iBAAA,CAAkB,SAAS,CAAA,EACjC;AACA,cAAA,MAAM,OAAA,GAAU,CAAA,UAAA,EAAa,KAAA,CAAM,EAAA,IAAM,WAAW,CAAA,CAAA;AAGpD,cAAA,IAAI,mBAAA,CAAoB,OAAO,CAAA,KAAM,KAAA,CAAM,iBAAA,EAAmB;AAC5D,gBAAA;AACF,cAAA;AACA,cAAA,mBAAA,CAAoB,OAAO,IAAI,KAAA,CAAM,iBAAA;AAErC,cAAA,IAAI,aAAA,CAAc,OAAO,CAAA,IAAK,IAAA,EAAM;AAClC,gBAAA,aAAA,CAAc,OAAO,CAAA,GAAI,EAAE,IAAA,EAAM,WAAA,EAAY;AAC7C,gBAAA,UAAA,CAAW,OAAA,CAAQ;kBACjB,IAAA,EAAM,iBAAA;kBACN,EAAA,EAAI;iBACL,CAAA;AACH,cAAA;AAEA,cAAA,UAAA,CAAW,OAAA,CAAQ;gBACjB,IAAA,EAAM,iBAAA;gBACN,EAAA,EAAI,OAAA;AACJ,gBAAA,KAAA,EAAO,KAAA,CAAM;eACd,CAAA;AACH,YAAA;AAGA,YAAA,IAAI,KAAA,CAAM,cAAc,IAAA,EAAM;AAC5B,cAAA,KAAA,MAAW,QAAA,IAAY,MAAM,UAAA,EAAY;AAEvC,gBAAA,MAAM,aAAa,QAAA,CAAS,EAAA;AAE5B,gBAAA,UAAA,CAAW,OAAA,CAAQ;kBACjB,IAAA,EAAM,kBAAA;kBACN,EAAA,EAAI,UAAA;AACJ,kBAAA,QAAA,EAAU,SAAS,QAAA,CAAS;iBAC7B,CAAA;AAED,gBAAA,UAAA,CAAW,OAAA,CAAQ;kBACjB,IAAA,EAAM,kBAAA;kBACN,EAAA,EAAI,UAAA;AACJ,kBAAA,KAAA,EAAO,SAAS,QAAA,CAAS;iBAC1B,CAAA;AAED,gBAAA,UAAA,CAAW,OAAA,CAAQ;kBACjB,IAAA,EAAM,gBAAA;kBACN,EAAA,EAAI;iBACL,CAAA;AAED,gBAAA,UAAA,CAAW,OAAA,CAAQ;kBACjB,IAAA,EAAM,WAAA;AACN,kBAAA,UAAA;AACA,kBAAA,QAAA,EAAU,SAAS,QAAA,CAAS,IAAA;AAC5B,kBAAA,KAAA,EAAO,SAAS,QAAA,CAAS;iBAC1B,CAAA;AACH,cAAA;AACF,YAAA;AACF,UAAA,CAAA;AAEA,UAAA,KAAA,CAAM,UAAA,EAAY;AAChB,YAAA,KAAA,MAAW,CAAC,OAAA,EAAS,KAAK,KAAK,MAAA,CAAO,OAAA,CAAQ,aAAa,CAAA,EAAG;AAC5D,cAAA,UAAA,CAAW,OAAA,CAAQ;gBACjB,IAAA,EAAM,KAAA,CAAM,IAAA,KAAS,MAAA,GAAS,UAAA,GAAa,eAAA;gBAC3C,EAAA,EAAI;eACL,CAAA;AACH,YAAA;AAEA,YAAA,UAAA,CAAW,QAAQ,EAAE,IAAA,EAAM,QAAA,EAAU,YAAA,EAAc,OAAO,CAAA;AAC5D,UAAA;SACD;AACH,OAAA;AACA,MAAA,OAAA,EAAS,EAAE,IAAA,EAAK;MAChB,QAAA,EAAU,EAAE,SAAS,eAAA;AACvB,KAAA;AACF,EAAA;AACF,CAAA;AAGA,IAAM,cAAA,GAAiBN,EAAE,MAAA,CAAO;AAC9B,EAAA,aAAA,EAAeA,EAAE,MAAA,EAAO;AACxB,EAAA,iBAAA,EAAmBA,EAAE,MAAA,EAAO;AAC5B,EAAA,YAAA,EAAcA,EAAE,MAAA,EAAO;AACvB,EAAA,yBAAA,EAA2BA,EACxB,MAAA,CAAO;IACN,gBAAA,EAAkBA,CAAAA,CAAE,MAAA,EAAO,CAAE,OAAA;AAC/B,GAAC,EACA,OAAA;AACL,CAAC,CAAA;AAED,IAAM,qBAAA,GAAwBA,EAAE,MAAA,CAAO;EACrC,EAAA,EAAIA,CAAAA,CAAE,MAAA,EAAO,CAAE,OAAA,EAAQ;EACvB,OAAA,EAASA,CAAAA,CAAE,MAAA,EAAO,CAAE,OAAA,EAAQ;EAC5B,KAAA,EAAOA,CAAAA,CAAE,MAAA,EAAO,CAAE,OAAA,EAAQ;AAC1B,EAAA,OAAA,EAASA,CAAAA,CAAE,KAAA;AACTA,IAAAA,CAAAA,CAAE,MAAA,CAAO;AACP,MAAA,OAAA,EAASA,EAAE,MAAA,CAAO;QAChB,IAAA,EAAMA,CAAAA,CAAE,QAAQ,WAAW,CAAA;QAC3B,OAAA,EAASA,CAAAA,CAAE,MAAA,EAAO,CAAE,OAAA,EAAQ;QAC5B,iBAAA,EAAmBA,CAAAA,CAAE,MAAA,EAAO,CAAE,OAAA,EAAQ;AACtC,QAAA,UAAA,EAAYA,CAAAA,CACT,KAAA;AACCA,UAAAA,CAAAA,CAAE,MAAA,CAAO;AACP,YAAA,EAAA,EAAIA,EAAE,MAAA,EAAO;YACb,IAAA,EAAMA,CAAAA,CAAE,QAAQ,UAAU,CAAA;AAC1B,YAAA,QAAA,EAAUA,EAAE,MAAA,CAAO;AACjB,cAAA,IAAA,EAAMA,EAAE,MAAA,EAAO;AACf,cAAA,SAAA,EAAWA,EAAE,MAAA;aACd;WACF;AACH,SAAA,CACC,OAAA;OACJ,CAAA;AACD,MAAA,KAAA,EAAOA,EAAE,MAAA,EAAO;MAChB,aAAA,EAAeA,CAAAA,CAAE,MAAA,EAAO,CAAE,OAAA;KAC3B;AACH,GAAA;EACA,MAAA,EAAQA,CAAAA,CAAE,QAAQ,iBAAiB,CAAA;EACnC,KAAA,EAAO,cAAA;EACP,SAAA,EAAWA,CAAAA,CAAE,MAAMA,CAAAA,CAAE,MAAA,GAAS,GAAA,EAAK,EAAE,OAAA;AACvC,CAAC,CAAA;AAED,IAAM,kBAAA,GAAqBA,EAAE,MAAA,CAAO;EAClC,EAAA,EAAIA,CAAAA,CAAE,MAAA,EAAO,CAAE,OAAA,EAAQ;EACvB,OAAA,EAASA,CAAAA,CAAE,MAAA,EAAO,CAAE,OAAA,EAAQ;EAC5B,KAAA,EAAOA,CAAAA,CAAE,MAAA,EAAO,CAAE,OAAA,EAAQ;AAC1B,EAAA,OAAA,EAASA,CAAAA,CAAE,KAAA;AACTA,IAAAA,CAAAA,CAAE,MAAA,CAAO;AACP,MAAA,KAAA,EAAOA,EAAE,MAAA,CAAO;AACd,QAAA,IAAA,EAAMA,EAAE,IAAA,CAAK,CAAC,WAAW,CAAC,EAAE,QAAA,EAAS;QACrC,OAAA,EAASA,CAAAA,CAAE,MAAA,EAAO,CAAE,OAAA,EAAQ;QAC5B,iBAAA,EAAmBA,CAAAA,CAAE,MAAA,EAAO,CAAE,OAAA,EAAQ;AACtC,QAAA,UAAA,EAAYA,CAAAA,CACT,KAAA;AACCA,UAAAA,CAAAA,CAAE,MAAA,CAAO;AACP,YAAA,EAAA,EAAIA,EAAE,MAAA,EAAO;YACb,IAAA,EAAMA,CAAAA,CAAE,QAAQ,UAAU,CAAA;AAC1B,YAAA,QAAA,EAAUA,EAAE,MAAA,CAAO;AACjB,cAAA,IAAA,EAAMA,EAAE,MAAA,EAAO;AACf,cAAA,SAAA,EAAWA,EAAE,MAAA;aACd;WACF;AACH,SAAA,CACC,OAAA;OACJ,CAAA;MACD,aAAA,EAAeA,CAAAA,CAAE,MAAA,EAAO,CAAE,OAAA,EAAQ;AAClC,MAAA,KAAA,EAAOA,EAAE,MAAA;KACV;AACH,GAAA;AACA,EAAA,KAAA,EAAO,eAAe,OAAA,EAAQ;EAC9B,SAAA,EAAWA,CAAAA,CAAE,MAAMA,CAAAA,CAAE,MAAA,GAAS,GAAA,EAAK,EAAE,OAAA;AACvC,CAAC,CAAA;ACtjBD,IAAM,iBAAA,GAA0D;EAC9D,WAAA,EAAa,kBAAA;EACb,cAAA,EAAgB,CAAA,IAAA,KAAQ,IAAA,CAAK,KAAA,CAAM;AACrC,CAAA;AAoDO,SAAS,SAAA,CAAU,OAAA,GAA+B,EAAC,EAAgB;AA5E1E,EAAA,IAAA,EAAA;AA6EE,EAAA,MAAM,OAAA,GAAU,oBAAA;KACd,EAAA,GAAA,OAAA,CAAQ,OAAA,KAAR,IAAA,GAAA,EAAA,GAAmB;AACrB,GAAA;AACA,EAAA,MAAM,aAAa,OAAO;AACxB,IAAA,aAAA,EAAe,UAAU,UAAA,CAAW;AAClC,MAAA,MAAA,EAAQ,OAAA,CAAQ,MAAA;MAChB,uBAAA,EAAyB,aAAA;MACzB,WAAA,EAAa;AACf,KAAC,CAAC,CAAA,CAAA;AACF,IAAA,GAAG,OAAA,CAAQ;AACb,GAAA,CAAA;AAEA,EAAA,MAAM,mBAAA,GAAsB,CAAC,OAAA,KAA4B;AACvD,IAAA,OAAO,IAAI,qBAAqB,OAAA,EAAS;MACvC,QAAA,EAAU,UAAA;AACV,MAAA,OAAA;MACA,OAAA,EAAS,UAAA;AACT,MAAA,UAAA;AACA,MAAA,KAAA,EAAO,OAAA,CAAQ;KAChB,CAAA;AACH,EAAA,CAAA;AAEA,EAAA,MAAM,gBAAA,GAAmB,CAAC,OAAA,KAA6B;AACrD,IAAA,OAAO,IAAI,2BAA2B,OAAA,EAAS;MAC7C,QAAA,EAAU,WAAA;AACV,MAAA,GAAA,EAAK,CAAC,EAAE,IAAA,OAAW,CAAA,EAAG,OAAO,GAAG,IAAI,CAAA,CAAA;MACpC,OAAA,EAAS,UAAA;AACT,MAAA,KAAA,EAAO,OAAA,CAAQ,KAAA;MACf,cAAA,EAAgB;KACjB,CAAA;AACH,EAAA,CAAA;AAEA,EAAA,MAAM,QAAA,GAAW,CAAC,OAAA,KAA4B,mBAAA,CAAoB,OAAO,CAAA;AAEzE,EAAA,QAAA,CAAS,aAAA,GAAgB,mBAAA;AACzB,EAAA,QAAA,CAAS,IAAA,GAAO,mBAAA;AAChB,EAAA,QAAA,CAAS,kBAAA,GAAqB,CAAC,OAAA,KAAoB;AACjD,IAAA,MAAM,IAAI,gBAAA,CAAiB,EAAE,OAAA,EAAS,SAAA,EAAW,sBAAsB,CAAA;AACzE,EAAA,CAAA;AACA,EAAA,QAAA,CAAS,UAAA,GAAa,gBAAA;AACtB,EAAA,QAAA,CAAS,KAAA,GAAQ,gBAAA;AAEjB,EAAA,OAAO,QAAA;AACT;AAEO,IAAM,MAAM,SAAA","file":"chunk-IOQGI4ML.js","sourcesContent":["import { z } from 'zod/v4';\n\nexport type OpenAICompatibleChatModelId = string;\n\nexport const openaiCompatibleProviderOptions = z.object({\n  /**\n   * A unique identifier representing your end-user, which can help the provider to\n   * monitor and detect abuse.\n   */\n  user: z.string().optional(),\n\n  /**\n   * Reasoning effort for reasoning models. Defaults to `medium`.\n   */\n  reasoningEffort: z.string().optional(),\n});\n\nexport type OpenAICompatibleProviderOptions = z.infer<\n  typeof openaiCompatibleProviderOptions\n>;\n","import { z, ZodType } from 'zod/v4';\n\nexport const openaiCompatibleErrorDataSchema = z.object({\n  error: z.object({\n    message: z.string(),\n\n    // The additional information below is handled loosely to support\n    // OpenAI-compatible providers that have slightly different error\n    // responses:\n    type: z.string().nullish(),\n    param: z.any().nullish(),\n    code: z.union([z.string(), z.number()]).nullish(),\n  }),\n});\n\nexport type OpenAICompatibleErrorData = z.infer<\n  typeof openaiCompatibleErrorDataSchema\n>;\n\nexport type ProviderErrorStructure<T> = {\n  errorSchema: ZodType<T>;\n  errorToMessage: (error: T) => string;\n  isRetryable?: (response: Response, error?: T) => boolean;\n};\n\nexport const defaultOpenAICompatibleErrorStructure: ProviderErrorStructure<OpenAICompatibleErrorData> =\n  {\n    errorSchema: openaiCompatibleErrorDataSchema,\n    errorToMessage: data => data.error.message,\n  };\n","import {\n  APICallError,\n  InvalidResponseDataError,\n  LanguageModelV2,\n  LanguageModelV2CallWarning,\n  LanguageModelV2Content,\n  LanguageModelV2FinishReason,\n  LanguageModelV2StreamPart,\n  SharedV2ProviderMetadata,\n} from '@ai-sdk/provider';\nimport {\n  combineHeaders,\n  createEventSourceResponseHandler,\n  createJsonErrorResponseHandler,\n  createJsonResponseHandler,\n  FetchFunction,\n  generateId,\n  isParsableJson,\n  parseProviderOptions,\n  ParseResult,\n  postJsonToApi,\n  ResponseHandler,\n} from '@ai-sdk/provider-utils';\nimport { z } from 'zod/v4';\nimport { convertToOpenAICompatibleChatMessages } from './convert-to-openai-compatible-chat-messages';\nimport { getResponseMetadata } from './get-response-metadata';\nimport { mapOpenAICompatibleFinishReason } from './map-openai-compatible-finish-reason';\nimport {\n  OpenAICompatibleChatModelId,\n  openaiCompatibleProviderOptions,\n} from './openai-compatible-chat-options';\nimport {\n  defaultOpenAICompatibleErrorStructure,\n  ProviderErrorStructure,\n} from './openai-compatible-error';\nimport { MetadataExtractor } from './openai-compatible-metadata-extractor';\nimport { prepareTools } from './openai-compatible-prepare-tools';\n\nexport type OpenAICompatibleChatConfig = {\n  provider: string;\n  headers: () => Record<string, string | undefined>;\n  url: (options: { modelId: string; path: string }) => string;\n  fetch?: FetchFunction;\n  includeUsage?: boolean;\n  errorStructure?: ProviderErrorStructure<any>;\n  metadataExtractor?: MetadataExtractor;\n\n  /**\n   * Whether the model supports structured outputs.\n   */\n  supportsStructuredOutputs?: boolean;\n\n  /**\n   * The supported URLs for the model.\n   */\n  supportedUrls?: () => LanguageModelV2['supportedUrls'];\n};\n\nexport class OpenAICompatibleChatLanguageModel implements LanguageModelV2 {\n  readonly specificationVersion = 'v2';\n\n  readonly supportsStructuredOutputs: boolean;\n\n  readonly modelId: OpenAICompatibleChatModelId;\n  private readonly config: OpenAICompatibleChatConfig;\n  private readonly failedResponseHandler: ResponseHandler<APICallError>;\n  private readonly chunkSchema; // type inferred via constructor\n\n  constructor(\n    modelId: OpenAICompatibleChatModelId,\n    config: OpenAICompatibleChatConfig,\n  ) {\n    this.modelId = modelId;\n    this.config = config;\n\n    // initialize error handling:\n    const errorStructure =\n      config.errorStructure ?? defaultOpenAICompatibleErrorStructure;\n    this.chunkSchema = createOpenAICompatibleChatChunkSchema(\n      errorStructure.errorSchema,\n    );\n    this.failedResponseHandler = createJsonErrorResponseHandler(errorStructure);\n\n    this.supportsStructuredOutputs = config.supportsStructuredOutputs ?? false;\n  }\n\n  get provider(): string {\n    return this.config.provider;\n  }\n\n  private get providerOptionsName(): string {\n    return this.config.provider.split('.')[0].trim();\n  }\n\n  get supportedUrls() {\n    return this.config.supportedUrls?.() ?? {};\n  }\n\n  private async getArgs({\n    prompt,\n    maxOutputTokens,\n    temperature,\n    topP,\n    topK,\n    frequencyPenalty,\n    presencePenalty,\n    providerOptions,\n    stopSequences,\n    responseFormat,\n    seed,\n    toolChoice,\n    tools,\n  }: Parameters<LanguageModelV2['doGenerate']>[0]) {\n    const warnings: LanguageModelV2CallWarning[] = [];\n\n    // Parse provider options\n    const compatibleOptions = Object.assign(\n      (await parseProviderOptions({\n        provider: 'openai-compatible',\n        providerOptions,\n        schema: openaiCompatibleProviderOptions,\n      })) ?? {},\n      (await parseProviderOptions({\n        provider: this.providerOptionsName,\n        providerOptions,\n        schema: openaiCompatibleProviderOptions,\n      })) ?? {},\n    );\n\n    if (topK != null) {\n      warnings.push({ type: 'unsupported-setting', setting: 'topK' });\n    }\n\n    if (\n      responseFormat?.type === 'json' &&\n      responseFormat.schema != null &&\n      !this.supportsStructuredOutputs\n    ) {\n      warnings.push({\n        type: 'unsupported-setting',\n        setting: 'responseFormat',\n        details:\n          'JSON response format schema is only supported with structuredOutputs',\n      });\n    }\n\n    const {\n      tools: openaiTools,\n      toolChoice: openaiToolChoice,\n      toolWarnings,\n    } = prepareTools({\n      tools,\n      toolChoice,\n    });\n\n    return {\n      args: {\n        // model id:\n        model: this.modelId,\n\n        // model specific settings:\n        user: compatibleOptions.user,\n\n        // standardized settings:\n        max_tokens: maxOutputTokens,\n        temperature,\n        top_p: topP,\n        frequency_penalty: frequencyPenalty,\n        presence_penalty: presencePenalty,\n        response_format:\n          responseFormat?.type === 'json'\n            ? this.supportsStructuredOutputs === true &&\n              responseFormat.schema != null\n              ? {\n                  type: 'json_schema',\n                  json_schema: {\n                    schema: responseFormat.schema,\n                    name: responseFormat.name ?? 'response',\n                    description: responseFormat.description,\n                  },\n                }\n              : { type: 'json_object' }\n            : undefined,\n\n        stop: stopSequences,\n        seed,\n        ...providerOptions?.[this.providerOptionsName],\n\n        reasoning_effort: compatibleOptions.reasoningEffort,\n\n        // messages:\n        messages: convertToOpenAICompatibleChatMessages(prompt),\n\n        // tools:\n        tools: openaiTools,\n        tool_choice: openaiToolChoice,\n      },\n      warnings: [...warnings, ...toolWarnings],\n    };\n  }\n\n  async doGenerate(\n    options: Parameters<LanguageModelV2['doGenerate']>[0],\n  ): Promise<Awaited<ReturnType<LanguageModelV2['doGenerate']>>> {\n    const { args, warnings } = await this.getArgs({ ...options });\n\n    const body = JSON.stringify(args);\n\n    const {\n      responseHeaders,\n      value: responseBody,\n      rawValue: rawResponse,\n    } = await postJsonToApi({\n      url: this.config.url({\n        path: '/chat/completions',\n        modelId: this.modelId,\n      }),\n      headers: combineHeaders(this.config.headers(), options.headers),\n      body: args,\n      failedResponseHandler: this.failedResponseHandler,\n      successfulResponseHandler: createJsonResponseHandler(\n        OpenAICompatibleChatResponseSchema,\n      ),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch,\n    });\n\n    const choice = responseBody.choices[0];\n    const content: Array<LanguageModelV2Content> = [];\n\n    // text content:\n    const text = choice.message.content;\n    if (text != null && text.length > 0) {\n      content.push({ type: 'text', text });\n    }\n\n    // reasoning content:\n    const reasoning =\n      choice.message.reasoning_content ?? choice.message.reasoning;\n    if (reasoning != null && reasoning.length > 0) {\n      content.push({\n        type: 'reasoning',\n        text: reasoning,\n      });\n    }\n\n    // tool calls:\n    if (choice.message.tool_calls != null) {\n      for (const toolCall of choice.message.tool_calls) {\n        content.push({\n          type: 'tool-call',\n          toolCallId: toolCall.id ?? generateId(),\n          toolName: toolCall.function.name,\n          input: toolCall.function.arguments!,\n        });\n      }\n    }\n\n    // provider metadata:\n    const providerMetadata: SharedV2ProviderMetadata = {\n      [this.providerOptionsName]: {},\n      ...(await this.config.metadataExtractor?.extractMetadata?.({\n        parsedBody: rawResponse,\n      })),\n    };\n    const completionTokenDetails =\n      responseBody.usage?.completion_tokens_details;\n    if (completionTokenDetails?.accepted_prediction_tokens != null) {\n      providerMetadata[this.providerOptionsName].acceptedPredictionTokens =\n        completionTokenDetails?.accepted_prediction_tokens;\n    }\n    if (completionTokenDetails?.rejected_prediction_tokens != null) {\n      providerMetadata[this.providerOptionsName].rejectedPredictionTokens =\n        completionTokenDetails?.rejected_prediction_tokens;\n    }\n\n    return {\n      content,\n      finishReason: mapOpenAICompatibleFinishReason(choice.finish_reason),\n      usage: {\n        inputTokens: responseBody.usage?.prompt_tokens ?? undefined,\n        outputTokens: responseBody.usage?.completion_tokens ?? undefined,\n        totalTokens: responseBody.usage?.total_tokens ?? undefined,\n        reasoningTokens:\n          responseBody.usage?.completion_tokens_details?.reasoning_tokens ??\n          undefined,\n        cachedInputTokens:\n          responseBody.usage?.prompt_tokens_details?.cached_tokens ?? undefined,\n      },\n      providerMetadata,\n      request: { body },\n      response: {\n        ...getResponseMetadata(responseBody),\n        headers: responseHeaders,\n        body: rawResponse,\n      },\n      warnings,\n    };\n  }\n\n  async doStream(\n    options: Parameters<LanguageModelV2['doStream']>[0],\n  ): Promise<Awaited<ReturnType<LanguageModelV2['doStream']>>> {\n    const { args, warnings } = await this.getArgs({ ...options });\n\n    const body = {\n      ...args,\n      stream: true,\n\n      // only include stream_options when in strict compatibility mode:\n      stream_options: this.config.includeUsage\n        ? { include_usage: true }\n        : undefined,\n    };\n\n    const metadataExtractor =\n      this.config.metadataExtractor?.createStreamExtractor();\n\n    const { responseHeaders, value: response } = await postJsonToApi({\n      url: this.config.url({\n        path: '/chat/completions',\n        modelId: this.modelId,\n      }),\n      headers: combineHeaders(this.config.headers(), options.headers),\n      body,\n      failedResponseHandler: this.failedResponseHandler,\n      successfulResponseHandler: createEventSourceResponseHandler(\n        this.chunkSchema,\n      ),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch,\n    });\n\n    const toolCalls: Array<{\n      id: string;\n      type: 'function';\n      function: {\n        name: string;\n        arguments: string;\n      };\n      hasFinished: boolean;\n    }> = [];\n\n    let finishReason: LanguageModelV2FinishReason = 'unknown';\n    const usage: {\n      completionTokens: number | undefined;\n      completionTokensDetails: {\n        reasoningTokens: number | undefined;\n        acceptedPredictionTokens: number | undefined;\n        rejectedPredictionTokens: number | undefined;\n      };\n      promptTokens: number | undefined;\n      promptTokensDetails: {\n        cachedTokens: number | undefined;\n      };\n      totalTokens: number | undefined;\n    } = {\n      completionTokens: undefined,\n      completionTokensDetails: {\n        reasoningTokens: undefined,\n        acceptedPredictionTokens: undefined,\n        rejectedPredictionTokens: undefined,\n      },\n      promptTokens: undefined,\n      promptTokensDetails: {\n        cachedTokens: undefined,\n      },\n      totalTokens: undefined,\n    };\n    let isFirstChunk = true;\n    const providerOptionsName = this.providerOptionsName;\n    let isActiveReasoning = false;\n    let isActiveText = false;\n\n    return {\n      stream: response.pipeThrough(\n        new TransformStream<\n          ParseResult<z.infer<typeof this.chunkSchema>>,\n          LanguageModelV2StreamPart\n        >({\n          start(controller) {\n            controller.enqueue({ type: 'stream-start', warnings });\n          },\n\n          // TODO we lost type safety on Chunk, most likely due to the error schema. MUST FIX\n          transform(chunk, controller) {\n            // Emit raw chunk if requested (before anything else)\n            if (options.includeRawChunks) {\n              controller.enqueue({ type: 'raw', rawValue: chunk.rawValue });\n            }\n\n            // handle failed chunk parsing / validation:\n            if (!chunk.success) {\n              finishReason = 'error';\n              controller.enqueue({ type: 'error', error: chunk.error });\n              return;\n            }\n            const value = chunk.value;\n\n            metadataExtractor?.processChunk(chunk.rawValue);\n\n            // handle error chunks:\n            if ('error' in value) {\n              finishReason = 'error';\n              controller.enqueue({ type: 'error', error: value.error.message });\n              return;\n            }\n\n            if (isFirstChunk) {\n              isFirstChunk = false;\n\n              controller.enqueue({\n                type: 'response-metadata',\n                ...getResponseMetadata(value),\n              });\n            }\n\n            if (value.usage != null) {\n              const {\n                prompt_tokens,\n                completion_tokens,\n                total_tokens,\n                prompt_tokens_details,\n                completion_tokens_details,\n              } = value.usage;\n\n              usage.promptTokens = prompt_tokens ?? undefined;\n              usage.completionTokens = completion_tokens ?? undefined;\n              usage.totalTokens = total_tokens ?? undefined;\n              if (completion_tokens_details?.reasoning_tokens != null) {\n                usage.completionTokensDetails.reasoningTokens =\n                  completion_tokens_details?.reasoning_tokens;\n              }\n              if (\n                completion_tokens_details?.accepted_prediction_tokens != null\n              ) {\n                usage.completionTokensDetails.acceptedPredictionTokens =\n                  completion_tokens_details?.accepted_prediction_tokens;\n              }\n              if (\n                completion_tokens_details?.rejected_prediction_tokens != null\n              ) {\n                usage.completionTokensDetails.rejectedPredictionTokens =\n                  completion_tokens_details?.rejected_prediction_tokens;\n              }\n              if (prompt_tokens_details?.cached_tokens != null) {\n                usage.promptTokensDetails.cachedTokens =\n                  prompt_tokens_details?.cached_tokens;\n              }\n            }\n\n            const choice = value.choices[0];\n\n            if (choice?.finish_reason != null) {\n              finishReason = mapOpenAICompatibleFinishReason(\n                choice.finish_reason,\n              );\n            }\n\n            if (choice?.delta == null) {\n              return;\n            }\n\n            const delta = choice.delta;\n\n            // enqueue reasoning before text deltas:\n            const reasoningContent = delta.reasoning_content ?? delta.reasoning;\n            if (reasoningContent) {\n              if (!isActiveReasoning) {\n                controller.enqueue({\n                  type: 'reasoning-start',\n                  id: 'reasoning-0',\n                });\n                isActiveReasoning = true;\n              }\n\n              controller.enqueue({\n                type: 'reasoning-delta',\n                id: 'reasoning-0',\n                delta: reasoningContent,\n              });\n            }\n\n            if (delta.content) {\n              if (!isActiveText) {\n                controller.enqueue({ type: 'text-start', id: 'txt-0' });\n                isActiveText = true;\n              }\n\n              controller.enqueue({\n                type: 'text-delta',\n                id: 'txt-0',\n                delta: delta.content,\n              });\n            }\n\n            if (delta.tool_calls != null) {\n              for (const toolCallDelta of delta.tool_calls) {\n                const index = toolCallDelta.index;\n\n                if (toolCalls[index] == null) {\n                  if (toolCallDelta.id == null) {\n                    throw new InvalidResponseDataError({\n                      data: toolCallDelta,\n                      message: `Expected 'id' to be a string.`,\n                    });\n                  }\n\n                  if (toolCallDelta.function?.name == null) {\n                    throw new InvalidResponseDataError({\n                      data: toolCallDelta,\n                      message: `Expected 'function.name' to be a string.`,\n                    });\n                  }\n\n                  controller.enqueue({\n                    type: 'tool-input-start',\n                    id: toolCallDelta.id,\n                    toolName: toolCallDelta.function.name,\n                  });\n\n                  toolCalls[index] = {\n                    id: toolCallDelta.id,\n                    type: 'function',\n                    function: {\n                      name: toolCallDelta.function.name,\n                      arguments: toolCallDelta.function.arguments ?? '',\n                    },\n                    hasFinished: false,\n                  };\n\n                  const toolCall = toolCalls[index];\n\n                  if (\n                    toolCall.function?.name != null &&\n                    toolCall.function?.arguments != null\n                  ) {\n                    // send delta if the argument text has already started:\n                    if (toolCall.function.arguments.length > 0) {\n                      controller.enqueue({\n                        type: 'tool-input-start',\n                        id: toolCall.id,\n                        toolName: toolCall.function.name,\n                      });\n                    }\n\n                    // check if tool call is complete\n                    // (some providers send the full tool call in one chunk):\n                    if (isParsableJson(toolCall.function.arguments)) {\n                      controller.enqueue({\n                        type: 'tool-input-end',\n                        id: toolCall.id,\n                      });\n\n                      controller.enqueue({\n                        type: 'tool-call',\n                        toolCallId: toolCall.id ?? generateId(),\n                        toolName: toolCall.function.name,\n                        input: toolCall.function.arguments,\n                      });\n                      toolCall.hasFinished = true;\n                    }\n                  }\n\n                  continue;\n                }\n\n                // existing tool call, merge if not finished\n                const toolCall = toolCalls[index];\n\n                if (toolCall.hasFinished) {\n                  continue;\n                }\n\n                if (toolCallDelta.function?.arguments != null) {\n                  toolCall.function!.arguments +=\n                    toolCallDelta.function?.arguments ?? '';\n                }\n\n                // send delta\n                controller.enqueue({\n                  type: 'tool-input-delta',\n                  id: toolCall.id,\n                  delta: toolCallDelta.function.arguments ?? '',\n                });\n\n                // check if tool call is complete\n                if (\n                  toolCall.function?.name != null &&\n                  toolCall.function?.arguments != null &&\n                  isParsableJson(toolCall.function.arguments)\n                ) {\n                  controller.enqueue({\n                    type: 'tool-input-end',\n                    id: toolCall.id,\n                  });\n\n                  controller.enqueue({\n                    type: 'tool-call',\n                    toolCallId: toolCall.id ?? generateId(),\n                    toolName: toolCall.function.name,\n                    input: toolCall.function.arguments,\n                  });\n                  toolCall.hasFinished = true;\n                }\n              }\n            }\n          },\n\n          flush(controller) {\n            if (isActiveReasoning) {\n              controller.enqueue({ type: 'reasoning-end', id: 'reasoning-0' });\n            }\n\n            if (isActiveText) {\n              controller.enqueue({ type: 'text-end', id: 'txt-0' });\n            }\n\n            // go through all tool calls and send the ones that are not finished\n            for (const toolCall of toolCalls.filter(\n              toolCall => !toolCall.hasFinished,\n            )) {\n              controller.enqueue({\n                type: 'tool-input-end',\n                id: toolCall.id,\n              });\n\n              controller.enqueue({\n                type: 'tool-call',\n                toolCallId: toolCall.id ?? generateId(),\n                toolName: toolCall.function.name,\n                input: toolCall.function.arguments,\n              });\n            }\n\n            const providerMetadata: SharedV2ProviderMetadata = {\n              [providerOptionsName]: {},\n              ...metadataExtractor?.buildMetadata(),\n            };\n            if (\n              usage.completionTokensDetails.acceptedPredictionTokens != null\n            ) {\n              providerMetadata[providerOptionsName].acceptedPredictionTokens =\n                usage.completionTokensDetails.acceptedPredictionTokens;\n            }\n            if (\n              usage.completionTokensDetails.rejectedPredictionTokens != null\n            ) {\n              providerMetadata[providerOptionsName].rejectedPredictionTokens =\n                usage.completionTokensDetails.rejectedPredictionTokens;\n            }\n\n            controller.enqueue({\n              type: 'finish',\n              finishReason,\n              usage: {\n                inputTokens: usage.promptTokens ?? undefined,\n                outputTokens: usage.completionTokens ?? undefined,\n                totalTokens: usage.totalTokens ?? undefined,\n                reasoningTokens:\n                  usage.completionTokensDetails.reasoningTokens ?? undefined,\n                cachedInputTokens:\n                  usage.promptTokensDetails.cachedTokens ?? undefined,\n              },\n              providerMetadata,\n            });\n          },\n        }),\n      ),\n      request: { body },\n      response: { headers: responseHeaders },\n    };\n  }\n}\n\nconst openaiCompatibleTokenUsageSchema = z\n  .object({\n    prompt_tokens: z.number().nullish(),\n    completion_tokens: z.number().nullish(),\n    total_tokens: z.number().nullish(),\n    prompt_tokens_details: z\n      .object({\n        cached_tokens: z.number().nullish(),\n      })\n      .nullish(),\n    completion_tokens_details: z\n      .object({\n        reasoning_tokens: z.number().nullish(),\n        accepted_prediction_tokens: z.number().nullish(),\n        rejected_prediction_tokens: z.number().nullish(),\n      })\n      .nullish(),\n  })\n  .nullish();\n\n// limited version of the schema, focussed on what is needed for the implementation\n// this approach limits breakages when the API changes and increases efficiency\nconst OpenAICompatibleChatResponseSchema = z.object({\n  id: z.string().nullish(),\n  created: z.number().nullish(),\n  model: z.string().nullish(),\n  choices: z.array(\n    z.object({\n      message: z.object({\n        role: z.literal('assistant').nullish(),\n        content: z.string().nullish(),\n        reasoning_content: z.string().nullish(),\n        reasoning: z.string().nullish(),\n        tool_calls: z\n          .array(\n            z.object({\n              id: z.string().nullish(),\n              function: z.object({\n                name: z.string(),\n                arguments: z.string(),\n              }),\n            }),\n          )\n          .nullish(),\n      }),\n      finish_reason: z.string().nullish(),\n    }),\n  ),\n  usage: openaiCompatibleTokenUsageSchema,\n});\n\n// limited version of the schema, focussed on what is needed for the implementation\n// this approach limits breakages when the API changes and increases efficiency\nconst createOpenAICompatibleChatChunkSchema = <\n  ERROR_SCHEMA extends z.core.$ZodType,\n>(\n  errorSchema: ERROR_SCHEMA,\n) =>\n  z.union([\n    z.object({\n      id: z.string().nullish(),\n      created: z.number().nullish(),\n      model: z.string().nullish(),\n      choices: z.array(\n        z.object({\n          delta: z\n            .object({\n              role: z.enum(['assistant']).nullish(),\n              content: z.string().nullish(),\n              // Most openai-compatible models set `reasoning_content`, but some\n              // providers serving `gpt-oss` set `reasoning`. See #7866\n              reasoning_content: z.string().nullish(),\n              reasoning: z.string().nullish(),\n              tool_calls: z\n                .array(\n                  z.object({\n                    index: z.number(),\n                    id: z.string().nullish(),\n                    function: z.object({\n                      name: z.string().nullish(),\n                      arguments: z.string().nullish(),\n                    }),\n                  }),\n                )\n                .nullish(),\n            })\n            .nullish(),\n          finish_reason: z.string().nullish(),\n        }),\n      ),\n      usage: openaiCompatibleTokenUsageSchema,\n    }),\n    errorSchema,\n  ]);\n","import { z } from 'zod/v4';\n\nexport type OpenAICompatibleCompletionModelId = string;\n\nexport const openaiCompatibleCompletionProviderOptions = z.object({\n  /**\n   * Echo back the prompt in addition to the completion.\n   */\n  echo: z.boolean().optional(),\n\n  /**\n   * Modify the likelihood of specified tokens appearing in the completion.\n   *\n   * Accepts a JSON object that maps tokens (specified by their token ID in\n   * the GPT tokenizer) to an associated bias value from -100 to 100.\n   */\n  logitBias: z.record(z.string(), z.number()).optional(),\n\n  /**\n   * The suffix that comes after a completion of inserted text.\n   */\n  suffix: z.string().optional(),\n\n  /**\n   * A unique identifier representing your end-user, which can help providers to\n   * monitor and detect abuse.\n   */\n  user: z.string().optional(),\n});\n\nexport type OpenAICompatibleCompletionProviderOptions = z.infer<\n  typeof openaiCompatibleCompletionProviderOptions\n>;\n","import {\n  APICallError,\n  LanguageModelV2,\n  LanguageModelV2CallWarning,\n  LanguageModelV2Content,\n  LanguageModelV2FinishReason,\n  LanguageModelV2StreamPart,\n  LanguageModelV2Usage,\n} from '@ai-sdk/provider';\nimport {\n  combineHeaders,\n  createEventSourceResponseHandler,\n  createJsonErrorResponseHandler,\n  createJsonResponseHandler,\n  FetchFunction,\n  parseProviderOptions,\n  ParseResult,\n  postJsonToApi,\n  ResponseHandler,\n} from '@ai-sdk/provider-utils';\nimport { z } from 'zod/v4';\nimport { convertToOpenAICompatibleCompletionPrompt } from './convert-to-openai-compatible-completion-prompt';\nimport { getResponseMetadata } from './get-response-metadata';\nimport { mapOpenAICompatibleFinishReason } from './map-openai-compatible-finish-reason';\nimport {\n  OpenAICompatibleCompletionModelId,\n  openaiCompatibleCompletionProviderOptions,\n} from './openai-compatible-completion-options';\nimport {\n  defaultOpenAICompatibleErrorStructure,\n  ProviderErrorStructure,\n} from './openai-compatible-error';\n\ntype OpenAICompatibleCompletionConfig = {\n  provider: string;\n  includeUsage?: boolean;\n  headers: () => Record<string, string | undefined>;\n  url: (options: { modelId: string; path: string }) => string;\n  fetch?: FetchFunction;\n  errorStructure?: ProviderErrorStructure<any>;\n\n  /**\n   * The supported URLs for the model.\n   */\n  supportedUrls?: () => LanguageModelV2['supportedUrls'];\n};\n\nexport class OpenAICompatibleCompletionLanguageModel\n  implements LanguageModelV2\n{\n  readonly specificationVersion = 'v2';\n\n  readonly modelId: OpenAICompatibleCompletionModelId;\n  private readonly config: OpenAICompatibleCompletionConfig;\n  private readonly failedResponseHandler: ResponseHandler<APICallError>;\n  private readonly chunkSchema; // type inferred via constructor\n\n  constructor(\n    modelId: OpenAICompatibleCompletionModelId,\n    config: OpenAICompatibleCompletionConfig,\n  ) {\n    this.modelId = modelId;\n    this.config = config;\n\n    // initialize error handling:\n    const errorStructure =\n      config.errorStructure ?? defaultOpenAICompatibleErrorStructure;\n    this.chunkSchema = createOpenAICompatibleCompletionChunkSchema(\n      errorStructure.errorSchema,\n    );\n    this.failedResponseHandler = createJsonErrorResponseHandler(errorStructure);\n  }\n\n  get provider(): string {\n    return this.config.provider;\n  }\n\n  private get providerOptionsName(): string {\n    return this.config.provider.split('.')[0].trim();\n  }\n\n  get supportedUrls() {\n    return this.config.supportedUrls?.() ?? {};\n  }\n\n  private async getArgs({\n    prompt,\n    maxOutputTokens,\n    temperature,\n    topP,\n    topK,\n    frequencyPenalty,\n    presencePenalty,\n    stopSequences: userStopSequences,\n    responseFormat,\n    seed,\n    providerOptions,\n    tools,\n    toolChoice,\n  }: Parameters<LanguageModelV2['doGenerate']>[0]) {\n    const warnings: LanguageModelV2CallWarning[] = [];\n\n    // Parse provider options\n    const completionOptions =\n      (await parseProviderOptions({\n        provider: this.providerOptionsName,\n        providerOptions,\n        schema: openaiCompatibleCompletionProviderOptions,\n      })) ?? {};\n\n    if (topK != null) {\n      warnings.push({ type: 'unsupported-setting', setting: 'topK' });\n    }\n\n    if (tools?.length) {\n      warnings.push({ type: 'unsupported-setting', setting: 'tools' });\n    }\n\n    if (toolChoice != null) {\n      warnings.push({ type: 'unsupported-setting', setting: 'toolChoice' });\n    }\n\n    if (responseFormat != null && responseFormat.type !== 'text') {\n      warnings.push({\n        type: 'unsupported-setting',\n        setting: 'responseFormat',\n        details: 'JSON response format is not supported.',\n      });\n    }\n\n    const { prompt: completionPrompt, stopSequences } =\n      convertToOpenAICompatibleCompletionPrompt({ prompt });\n\n    const stop = [...(stopSequences ?? []), ...(userStopSequences ?? [])];\n\n    return {\n      args: {\n        // model id:\n        model: this.modelId,\n\n        // model specific settings:\n        echo: completionOptions.echo,\n        logit_bias: completionOptions.logitBias,\n        suffix: completionOptions.suffix,\n        user: completionOptions.user,\n\n        // standardized settings:\n        max_tokens: maxOutputTokens,\n        temperature,\n        top_p: topP,\n        frequency_penalty: frequencyPenalty,\n        presence_penalty: presencePenalty,\n        seed,\n        ...providerOptions?.[this.providerOptionsName],\n\n        // prompt:\n        prompt: completionPrompt,\n\n        // stop sequences:\n        stop: stop.length > 0 ? stop : undefined,\n      },\n      warnings,\n    };\n  }\n\n  async doGenerate(\n    options: Parameters<LanguageModelV2['doGenerate']>[0],\n  ): Promise<Awaited<ReturnType<LanguageModelV2['doGenerate']>>> {\n    const { args, warnings } = await this.getArgs(options);\n\n    const {\n      responseHeaders,\n      value: response,\n      rawValue: rawResponse,\n    } = await postJsonToApi({\n      url: this.config.url({\n        path: '/completions',\n        modelId: this.modelId,\n      }),\n      headers: combineHeaders(this.config.headers(), options.headers),\n      body: args,\n      failedResponseHandler: this.failedResponseHandler,\n      successfulResponseHandler: createJsonResponseHandler(\n        openaiCompatibleCompletionResponseSchema,\n      ),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch,\n    });\n\n    const choice = response.choices[0];\n    const content: Array<LanguageModelV2Content> = [];\n\n    // text content:\n    if (choice.text != null && choice.text.length > 0) {\n      content.push({ type: 'text', text: choice.text });\n    }\n\n    return {\n      content,\n      usage: {\n        inputTokens: response.usage?.prompt_tokens ?? undefined,\n        outputTokens: response.usage?.completion_tokens ?? undefined,\n        totalTokens: response.usage?.total_tokens ?? undefined,\n      },\n      finishReason: mapOpenAICompatibleFinishReason(choice.finish_reason),\n      request: { body: args },\n      response: {\n        ...getResponseMetadata(response),\n        headers: responseHeaders,\n        body: rawResponse,\n      },\n      warnings,\n    };\n  }\n\n  async doStream(\n    options: Parameters<LanguageModelV2['doStream']>[0],\n  ): Promise<Awaited<ReturnType<LanguageModelV2['doStream']>>> {\n    const { args, warnings } = await this.getArgs(options);\n\n    const body = {\n      ...args,\n      stream: true,\n\n      // only include stream_options when in strict compatibility mode:\n      stream_options: this.config.includeUsage\n        ? { include_usage: true }\n        : undefined,\n    };\n\n    const { responseHeaders, value: response } = await postJsonToApi({\n      url: this.config.url({\n        path: '/completions',\n        modelId: this.modelId,\n      }),\n      headers: combineHeaders(this.config.headers(), options.headers),\n      body,\n      failedResponseHandler: this.failedResponseHandler,\n      successfulResponseHandler: createEventSourceResponseHandler(\n        this.chunkSchema,\n      ),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch,\n    });\n\n    let finishReason: LanguageModelV2FinishReason = 'unknown';\n    const usage: LanguageModelV2Usage = {\n      inputTokens: undefined,\n      outputTokens: undefined,\n      totalTokens: undefined,\n    };\n    let isFirstChunk = true;\n\n    return {\n      stream: response.pipeThrough(\n        new TransformStream<\n          ParseResult<z.infer<typeof this.chunkSchema>>,\n          LanguageModelV2StreamPart\n        >({\n          start(controller) {\n            controller.enqueue({ type: 'stream-start', warnings });\n          },\n\n          transform(chunk, controller) {\n            if (options.includeRawChunks) {\n              controller.enqueue({ type: 'raw', rawValue: chunk.rawValue });\n            }\n\n            // handle failed chunk parsing / validation:\n            if (!chunk.success) {\n              finishReason = 'error';\n              controller.enqueue({ type: 'error', error: chunk.error });\n              return;\n            }\n\n            const value = chunk.value;\n\n            // handle error chunks:\n            if ('error' in value) {\n              finishReason = 'error';\n              controller.enqueue({ type: 'error', error: value.error });\n              return;\n            }\n\n            if (isFirstChunk) {\n              isFirstChunk = false;\n\n              controller.enqueue({\n                type: 'response-metadata',\n                ...getResponseMetadata(value),\n              });\n\n              controller.enqueue({\n                type: 'text-start',\n                id: '0',\n              });\n            }\n\n            if (value.usage != null) {\n              usage.inputTokens = value.usage.prompt_tokens ?? undefined;\n              usage.outputTokens = value.usage.completion_tokens ?? undefined;\n              usage.totalTokens = value.usage.total_tokens ?? undefined;\n            }\n\n            const choice = value.choices[0];\n\n            if (choice?.finish_reason != null) {\n              finishReason = mapOpenAICompatibleFinishReason(\n                choice.finish_reason,\n              );\n            }\n\n            if (choice?.text != null) {\n              controller.enqueue({\n                type: 'text-delta',\n                id: '0',\n                delta: choice.text,\n              });\n            }\n          },\n\n          flush(controller) {\n            if (!isFirstChunk) {\n              controller.enqueue({ type: 'text-end', id: '0' });\n            }\n\n            controller.enqueue({\n              type: 'finish',\n              finishReason,\n              usage,\n            });\n          },\n        }),\n      ),\n      request: { body },\n      response: { headers: responseHeaders },\n    };\n  }\n}\n\nconst usageSchema = z.object({\n  prompt_tokens: z.number(),\n  completion_tokens: z.number(),\n  total_tokens: z.number(),\n});\n\n// limited version of the schema, focussed on what is needed for the implementation\n// this approach limits breakages when the API changes and increases efficiency\nconst openaiCompatibleCompletionResponseSchema = z.object({\n  id: z.string().nullish(),\n  created: z.number().nullish(),\n  model: z.string().nullish(),\n  choices: z.array(\n    z.object({\n      text: z.string(),\n      finish_reason: z.string(),\n    }),\n  ),\n  usage: usageSchema.nullish(),\n});\n\n// limited version of the schema, focussed on what is needed for the implementation\n// this approach limits breakages when the API changes and increases efficiency\nconst createOpenAICompatibleCompletionChunkSchema = <\n  ERROR_SCHEMA extends z.core.$ZodType,\n>(\n  errorSchema: ERROR_SCHEMA,\n) =>\n  z.union([\n    z.object({\n      id: z.string().nullish(),\n      created: z.number().nullish(),\n      model: z.string().nullish(),\n      choices: z.array(\n        z.object({\n          text: z.string(),\n          finish_reason: z.string().nullish(),\n          index: z.number(),\n        }),\n      ),\n      usage: usageSchema.nullish(),\n    }),\n    errorSchema,\n  ]);\n","import { z } from 'zod/v4';\n\nexport type OpenAICompatibleEmbeddingModelId = string;\n\nexport const openaiCompatibleEmbeddingProviderOptions = z.object({\n  /**\n   * The number of dimensions the resulting output embeddings should have.\n   * Only supported in text-embedding-3 and later models.\n   */\n  dimensions: z.number().optional(),\n\n  /**\n   * A unique identifier representing your end-user, which can help providers to\n   * monitor and detect abuse.\n   */\n  user: z.string().optional(),\n});\n\nexport type OpenAICompatibleEmbeddingProviderOptions = z.infer<\n  typeof openaiCompatibleEmbeddingProviderOptions\n>;\n","import {\n  EmbeddingModelV2,\n  TooManyEmbeddingValuesForCallError,\n} from '@ai-sdk/provider';\nimport {\n  combineHeaders,\n  createJsonErrorResponseHandler,\n  createJsonResponseHandler,\n  FetchFunction,\n  parseProviderOptions,\n  postJsonToApi,\n} from '@ai-sdk/provider-utils';\nimport { z } from 'zod/v4';\nimport {\n  OpenAICompatibleEmbeddingModelId,\n  openaiCompatibleEmbeddingProviderOptions,\n} from './openai-compatible-embedding-options';\nimport {\n  defaultOpenAICompatibleErrorStructure,\n  ProviderErrorStructure,\n} from './openai-compatible-error';\n\ntype OpenAICompatibleEmbeddingConfig = {\n  /**\nOverride the maximum number of embeddings per call.\n   */\n  maxEmbeddingsPerCall?: number;\n\n  /**\nOverride the parallelism of embedding calls.\n  */\n  supportsParallelCalls?: boolean;\n\n  provider: string;\n  url: (options: { modelId: string; path: string }) => string;\n  headers: () => Record<string, string | undefined>;\n  fetch?: FetchFunction;\n  errorStructure?: ProviderErrorStructure<any>;\n};\n\nexport class OpenAICompatibleEmbeddingModel\n  implements EmbeddingModelV2<string>\n{\n  readonly specificationVersion = 'v2';\n  readonly modelId: OpenAICompatibleEmbeddingModelId;\n\n  private readonly config: OpenAICompatibleEmbeddingConfig;\n\n  get provider(): string {\n    return this.config.provider;\n  }\n\n  get maxEmbeddingsPerCall(): number {\n    return this.config.maxEmbeddingsPerCall ?? 2048;\n  }\n\n  get supportsParallelCalls(): boolean {\n    return this.config.supportsParallelCalls ?? true;\n  }\n\n  constructor(\n    modelId: OpenAICompatibleEmbeddingModelId,\n    config: OpenAICompatibleEmbeddingConfig,\n  ) {\n    this.modelId = modelId;\n    this.config = config;\n  }\n\n  private get providerOptionsName(): string {\n    return this.config.provider.split('.')[0].trim();\n  }\n\n  async doEmbed({\n    values,\n    headers,\n    abortSignal,\n    providerOptions,\n  }: Parameters<EmbeddingModelV2<string>['doEmbed']>[0]): Promise<\n    Awaited<ReturnType<EmbeddingModelV2<string>['doEmbed']>>\n  > {\n    const compatibleOptions = Object.assign(\n      (await parseProviderOptions({\n        provider: 'openai-compatible',\n        providerOptions,\n        schema: openaiCompatibleEmbeddingProviderOptions,\n      })) ?? {},\n      (await parseProviderOptions({\n        provider: this.providerOptionsName,\n        providerOptions,\n        schema: openaiCompatibleEmbeddingProviderOptions,\n      })) ?? {},\n    );\n\n    if (values.length > this.maxEmbeddingsPerCall) {\n      throw new TooManyEmbeddingValuesForCallError({\n        provider: this.provider,\n        modelId: this.modelId,\n        maxEmbeddingsPerCall: this.maxEmbeddingsPerCall,\n        values,\n      });\n    }\n\n    const {\n      responseHeaders,\n      value: response,\n      rawValue,\n    } = await postJsonToApi({\n      url: this.config.url({\n        path: '/embeddings',\n        modelId: this.modelId,\n      }),\n      headers: combineHeaders(this.config.headers(), headers),\n      body: {\n        model: this.modelId,\n        input: values,\n        encoding_format: 'float',\n        dimensions: compatibleOptions.dimensions,\n        user: compatibleOptions.user,\n      },\n      failedResponseHandler: createJsonErrorResponseHandler(\n        this.config.errorStructure ?? defaultOpenAICompatibleErrorStructure,\n      ),\n      successfulResponseHandler: createJsonResponseHandler(\n        openaiTextEmbeddingResponseSchema,\n      ),\n      abortSignal,\n      fetch: this.config.fetch,\n    });\n\n    return {\n      embeddings: response.data.map(item => item.embedding),\n      usage: response.usage\n        ? { tokens: response.usage.prompt_tokens }\n        : undefined,\n      providerMetadata: response.providerMetadata,\n      response: { headers: responseHeaders, body: rawValue },\n    };\n  }\n}\n\n// minimal version of the schema, focussed on what is needed for the implementation\n// this approach limits breakages when the API changes and increases efficiency\nconst openaiTextEmbeddingResponseSchema = z.object({\n  data: z.array(z.object({ embedding: z.array(z.number()) })),\n  usage: z.object({ prompt_tokens: z.number() }).nullish(),\n  providerMetadata: z\n    .record(z.string(), z.record(z.string(), z.any()))\n    .optional(),\n});\n","import { ImageModelV2, ImageModelV2CallWarning } from '@ai-sdk/provider';\nimport {\n  combineHeaders,\n  createJsonErrorResponseHandler,\n  createJsonResponseHandler,\n  FetchFunction,\n  postJsonToApi,\n} from '@ai-sdk/provider-utils';\nimport { z } from 'zod/v4';\nimport {\n  defaultOpenAICompatibleErrorStructure,\n  ProviderErrorStructure,\n} from './openai-compatible-error';\nimport { OpenAICompatibleImageModelId } from './openai-compatible-image-settings';\n\nexport type OpenAICompatibleImageModelConfig = {\n  provider: string;\n  headers: () => Record<string, string | undefined>;\n  url: (options: { modelId: string; path: string }) => string;\n  fetch?: FetchFunction;\n  errorStructure?: ProviderErrorStructure<any>;\n  _internal?: {\n    currentDate?: () => Date;\n  };\n};\n\nexport class OpenAICompatibleImageModel implements ImageModelV2 {\n  readonly specificationVersion = 'v2';\n  readonly maxImagesPerCall = 10;\n\n  get provider(): string {\n    return this.config.provider;\n  }\n\n  constructor(\n    readonly modelId: OpenAICompatibleImageModelId,\n    private readonly config: OpenAICompatibleImageModelConfig,\n  ) {}\n\n  async doGenerate({\n    prompt,\n    n,\n    size,\n    aspectRatio,\n    seed,\n    providerOptions,\n    headers,\n    abortSignal,\n  }: Parameters<ImageModelV2['doGenerate']>[0]): Promise<\n    Awaited<ReturnType<ImageModelV2['doGenerate']>>\n  > {\n    const warnings: Array<ImageModelV2CallWarning> = [];\n\n    if (aspectRatio != null) {\n      warnings.push({\n        type: 'unsupported-setting',\n        setting: 'aspectRatio',\n        details:\n          'This model does not support aspect ratio. Use `size` instead.',\n      });\n    }\n\n    if (seed != null) {\n      warnings.push({ type: 'unsupported-setting', setting: 'seed' });\n    }\n\n    const currentDate = this.config._internal?.currentDate?.() ?? new Date();\n    const { value: response, responseHeaders } = await postJsonToApi({\n      url: this.config.url({\n        path: '/images/generations',\n        modelId: this.modelId,\n      }),\n      headers: combineHeaders(this.config.headers(), headers),\n      body: {\n        model: this.modelId,\n        prompt,\n        n,\n        size,\n        ...(providerOptions.openai ?? {}),\n        response_format: 'b64_json',\n      },\n      failedResponseHandler: createJsonErrorResponseHandler(\n        this.config.errorStructure ?? defaultOpenAICompatibleErrorStructure,\n      ),\n      successfulResponseHandler: createJsonResponseHandler(\n        openaiCompatibleImageResponseSchema,\n      ),\n      abortSignal,\n      fetch: this.config.fetch,\n    });\n\n    return {\n      images: response.data.map(item => item.b64_json),\n      warnings,\n      response: {\n        timestamp: currentDate,\n        modelId: this.modelId,\n        headers: responseHeaders,\n      },\n    };\n  }\n}\n\n// minimal version of the schema, focussed on what is needed for the implementation\n// this approach limits breakages when the API changes and increases efficiency\nconst openaiCompatibleImageResponseSchema = z.object({\n  data: z.array(z.object({ b64_json: z.string() })),\n});\n","import {\n  LanguageModelV2CallWarning,\n  LanguageModelV2Prompt,\n  UnsupportedFunctionalityError,\n} from '@ai-sdk/provider';\nimport { convertToBase64 } from '@ai-sdk/provider-utils';\nimport { XaiChatPrompt } from './xai-chat-prompt';\n\nexport function convertToXaiChatMessages(prompt: LanguageModelV2Prompt): {\n  messages: XaiChatPrompt;\n  warnings: Array<LanguageModelV2CallWarning>;\n} {\n  const messages: XaiChatPrompt = [];\n  const warnings: Array<LanguageModelV2CallWarning> = [];\n\n  for (const { role, content } of prompt) {\n    switch (role) {\n      case 'system': {\n        messages.push({ role: 'system', content });\n        break;\n      }\n\n      case 'user': {\n        if (content.length === 1 && content[0].type === 'text') {\n          messages.push({ role: 'user', content: content[0].text });\n          break;\n        }\n\n        messages.push({\n          role: 'user',\n          content: content.map(part => {\n            switch (part.type) {\n              case 'text': {\n                return { type: 'text', text: part.text };\n              }\n              case 'file': {\n                if (part.mediaType.startsWith('image/')) {\n                  const mediaType =\n                    part.mediaType === 'image/*'\n                      ? 'image/jpeg'\n                      : part.mediaType;\n\n                  return {\n                    type: 'image_url',\n                    image_url: {\n                      url:\n                        part.data instanceof URL\n                          ? part.data.toString()\n                          : `data:${mediaType};base64,${convertToBase64(part.data)}`,\n                    },\n                  };\n                } else {\n                  throw new UnsupportedFunctionalityError({\n                    functionality: `file part media type ${part.mediaType}`,\n                  });\n                }\n              }\n            }\n          }),\n        });\n\n        break;\n      }\n\n      case 'assistant': {\n        let text = '';\n        const toolCalls: Array<{\n          id: string;\n          type: 'function';\n          function: { name: string; arguments: string };\n        }> = [];\n\n        for (const part of content) {\n          switch (part.type) {\n            case 'text': {\n              text += part.text;\n              break;\n            }\n            case 'tool-call': {\n              toolCalls.push({\n                id: part.toolCallId,\n                type: 'function',\n                function: {\n                  name: part.toolName,\n                  arguments: JSON.stringify(part.input),\n                },\n              });\n              break;\n            }\n          }\n        }\n\n        messages.push({\n          role: 'assistant',\n          content: text,\n          tool_calls: toolCalls.length > 0 ? toolCalls : undefined,\n        });\n\n        break;\n      }\n\n      case 'tool': {\n        for (const toolResponse of content) {\n          const output = toolResponse.output;\n\n          let contentValue: string;\n          switch (output.type) {\n            case 'text':\n            case 'error-text':\n              contentValue = output.value;\n              break;\n            case 'content':\n            case 'json':\n            case 'error-json':\n              contentValue = JSON.stringify(output.value);\n              break;\n          }\n\n          messages.push({\n            role: 'tool',\n            tool_call_id: toolResponse.toolCallId,\n            content: contentValue,\n          });\n        }\n        break;\n      }\n\n      default: {\n        const _exhaustiveCheck: never = role;\n        throw new Error(`Unsupported role: ${_exhaustiveCheck}`);\n      }\n    }\n  }\n\n  return { messages, warnings };\n}\n","export function getResponseMetadata({\n  id,\n  model,\n  created,\n}: {\n  id?: string | undefined | null;\n  created?: number | undefined | null;\n  model?: string | undefined | null;\n}) {\n  return {\n    id: id ?? undefined,\n    modelId: model ?? undefined,\n    timestamp: created != null ? new Date(created * 1000) : undefined,\n  };\n}\n","import { LanguageModelV2FinishReason } from '@ai-sdk/provider';\n\nexport function mapXaiFinishReason(\n  finishReason: string | null | undefined,\n): LanguageModelV2FinishReason {\n  switch (finishReason) {\n    case 'stop':\n      return 'stop';\n    case 'length':\n      return 'length';\n    case 'tool_calls':\n    case 'function_call':\n      return 'tool-calls';\n    case 'content_filter':\n      return 'content-filter';\n    default:\n      return 'unknown';\n  }\n}\n","import { z } from 'zod/v4';\n\n// https://console.x.ai and see \"View models\"\nexport type XaiChatModelId =\n  | 'grok-4'\n  | 'grok-4-0709'\n  | 'grok-4-latest'\n  | 'grok-3'\n  | 'grok-3-latest'\n  | 'grok-3-fast'\n  | 'grok-3-fast-latest'\n  | 'grok-3-mini'\n  | 'grok-3-mini-latest'\n  | 'grok-3-mini-fast'\n  | 'grok-3-mini-fast-latest'\n  | 'grok-2-vision-1212'\n  | 'grok-2-vision'\n  | 'grok-2-vision-latest'\n  | 'grok-2-image-1212'\n  | 'grok-2-image'\n  | 'grok-2-image-latest'\n  | 'grok-2-1212'\n  | 'grok-2'\n  | 'grok-2-latest'\n  | 'grok-vision-beta'\n  | 'grok-beta'\n  | (string & {});\n\n// search source schemas\nconst webSourceSchema = z.object({\n  type: z.literal('web'),\n  country: z.string().length(2).optional(),\n  excludedWebsites: z.array(z.string()).max(5).optional(),\n  allowedWebsites: z.array(z.string()).max(5).optional(),\n  safeSearch: z.boolean().optional(),\n});\n\nconst xSourceSchema = z.object({\n  type: z.literal('x'),\n  xHandles: z.array(z.string()).optional(),\n});\n\nconst newsSourceSchema = z.object({\n  type: z.literal('news'),\n  country: z.string().length(2).optional(),\n  excludedWebsites: z.array(z.string()).max(5).optional(),\n  safeSearch: z.boolean().optional(),\n});\n\nconst rssSourceSchema = z.object({\n  type: z.literal('rss'),\n  links: z.array(z.string().url()).max(1), // currently only supports one RSS link\n});\n\nconst searchSourceSchema = z.discriminatedUnion('type', [\n  webSourceSchema,\n  xSourceSchema,\n  newsSourceSchema,\n  rssSourceSchema,\n]);\n\n// xai-specific provider options\nexport const xaiProviderOptions = z.object({\n  /**\n   * reasoning effort for reasoning models\n   * only supported by grok-3-mini and grok-3-mini-fast models\n   */\n  reasoningEffort: z.enum(['low', 'high']).optional(),\n\n  searchParameters: z\n    .object({\n      /**\n       * search mode preference\n       * - \"off\": disables search completely\n       * - \"auto\": model decides whether to search (default)\n       * - \"on\": always enables search\n       */\n      mode: z.enum(['off', 'auto', 'on']),\n\n      /**\n       * whether to return citations in the response\n       * defaults to true\n       */\n      returnCitations: z.boolean().optional(),\n\n      /**\n       * start date for search data (ISO8601 format: YYYY-MM-DD)\n       */\n      fromDate: z.string().optional(),\n\n      /**\n       * end date for search data (ISO8601 format: YYYY-MM-DD)\n       */\n      toDate: z.string().optional(),\n\n      /**\n       * maximum number of search results to consider\n       * defaults to 20\n       */\n      maxSearchResults: z.number().min(1).max(50).optional(),\n\n      /**\n       * data sources to search from\n       * defaults to [\"web\", \"x\"] if not specified\n       */\n      sources: z.array(searchSourceSchema).optional(),\n    })\n    .optional(),\n});\n\nexport type XaiProviderOptions = z.infer<typeof xaiProviderOptions>;\n","import { createJsonErrorResponseHandler } from '@ai-sdk/provider-utils';\nimport { z } from 'zod/v4';\n\n// Add error schema and structure\nexport const xaiErrorDataSchema = z.object({\n  error: z.object({\n    message: z.string(),\n    type: z.string().nullish(),\n    param: z.any().nullish(),\n    code: z.union([z.string(), z.number()]).nullish(),\n  }),\n});\n\nexport type XaiErrorData = z.infer<typeof xaiErrorDataSchema>;\n\nexport const xaiFailedResponseHandler = createJsonErrorResponseHandler({\n  errorSchema: xaiErrorDataSchema,\n  errorToMessage: data => data.error.message,\n});\n","import {\n  LanguageModelV2CallOptions,\n  LanguageModelV2CallWarning,\n  UnsupportedFunctionalityError,\n} from '@ai-sdk/provider';\nimport { XaiToolChoice } from './xai-chat-prompt';\n\nexport function prepareTools({\n  tools,\n  toolChoice,\n}: {\n  tools: LanguageModelV2CallOptions['tools'];\n  toolChoice?: LanguageModelV2CallOptions['toolChoice'];\n}): {\n  tools:\n    | Array<{\n        type: 'function';\n        function: {\n          name: string;\n          description: string | undefined;\n          parameters: unknown;\n        };\n      }>\n    | undefined;\n  toolChoice: XaiToolChoice | undefined;\n  toolWarnings: LanguageModelV2CallWarning[];\n} {\n  // when the tools array is empty, change it to undefined to prevent errors\n  tools = tools?.length ? tools : undefined;\n\n  const toolWarnings: LanguageModelV2CallWarning[] = [];\n\n  if (tools == null) {\n    return { tools: undefined, toolChoice: undefined, toolWarnings };\n  }\n\n  // convert ai sdk tools to xai format\n  const xaiTools: Array<{\n    type: 'function';\n    function: {\n      name: string;\n      description: string | undefined;\n      parameters: unknown;\n    };\n  }> = [];\n\n  for (const tool of tools) {\n    if (tool.type === 'provider-defined') {\n      toolWarnings.push({ type: 'unsupported-tool', tool });\n    } else {\n      xaiTools.push({\n        type: 'function',\n        function: {\n          name: tool.name,\n          description: tool.description,\n          parameters: tool.inputSchema,\n        },\n      });\n    }\n  }\n\n  if (toolChoice == null) {\n    return { tools: xaiTools, toolChoice: undefined, toolWarnings };\n  }\n\n  const type = toolChoice.type;\n\n  switch (type) {\n    case 'auto':\n    case 'none':\n      return { tools: xaiTools, toolChoice: type, toolWarnings };\n    case 'required':\n      // xai supports 'required' directly\n      return { tools: xaiTools, toolChoice: 'required', toolWarnings };\n    case 'tool':\n      // xai supports specific tool selection\n      return {\n        tools: xaiTools,\n        toolChoice: {\n          type: 'function',\n          function: { name: toolChoice.toolName },\n        },\n        toolWarnings,\n      };\n    default: {\n      const _exhaustiveCheck: never = type;\n      throw new UnsupportedFunctionalityError({\n        functionality: `tool choice type: ${_exhaustiveCheck}`,\n      });\n    }\n  }\n}\n","import {\n  LanguageModelV2,\n  LanguageModelV2CallWarning,\n  LanguageModelV2Content,\n  LanguageModelV2FinishReason,\n  LanguageModelV2StreamPart,\n  LanguageModelV2Usage,\n} from '@ai-sdk/provider';\nimport {\n  combineHeaders,\n  createEventSourceResponseHandler,\n  createJsonResponseHandler,\n  FetchFunction,\n  parseProviderOptions,\n  ParseResult,\n  postJsonToApi,\n} from '@ai-sdk/provider-utils';\nimport { z } from 'zod/v4';\nimport { convertToXaiChatMessages } from './convert-to-xai-chat-messages';\nimport { getResponseMetadata } from './get-response-metadata';\nimport { mapXaiFinishReason } from './map-xai-finish-reason';\nimport { XaiChatModelId, xaiProviderOptions } from './xai-chat-options';\nimport { xaiFailedResponseHandler } from './xai-error';\nimport { prepareTools } from './xai-prepare-tools';\n\ntype XaiChatConfig = {\n  provider: string;\n  baseURL: string | undefined;\n  headers: () => Record<string, string | undefined>;\n  generateId: () => string;\n  fetch?: FetchFunction;\n};\n\nexport class XaiChatLanguageModel implements LanguageModelV2 {\n  readonly specificationVersion = 'v2';\n\n  readonly modelId: XaiChatModelId;\n\n  private readonly config: XaiChatConfig;\n\n  constructor(modelId: XaiChatModelId, config: XaiChatConfig) {\n    this.modelId = modelId;\n    this.config = config;\n  }\n\n  get provider(): string {\n    return this.config.provider;\n  }\n\n  readonly supportedUrls: Record<string, RegExp[]> = {\n    'image/*': [/^https?:\\/\\/.*$/],\n  };\n\n  private async getArgs({\n    prompt,\n    maxOutputTokens,\n    temperature,\n    topP,\n    topK,\n    frequencyPenalty,\n    presencePenalty,\n    stopSequences,\n    seed,\n    responseFormat,\n    providerOptions,\n    tools,\n    toolChoice,\n  }: Parameters<LanguageModelV2['doGenerate']>[0]) {\n    const warnings: LanguageModelV2CallWarning[] = [];\n\n    // parse xai-specific provider options\n    const options =\n      (await parseProviderOptions({\n        provider: 'xai',\n        providerOptions,\n        schema: xaiProviderOptions,\n      })) ?? {};\n\n    // check for unsupported parameters\n    if (topK != null) {\n      warnings.push({\n        type: 'unsupported-setting',\n        setting: 'topK',\n      });\n    }\n\n    if (frequencyPenalty != null) {\n      warnings.push({\n        type: 'unsupported-setting',\n        setting: 'frequencyPenalty',\n      });\n    }\n\n    if (presencePenalty != null) {\n      warnings.push({\n        type: 'unsupported-setting',\n        setting: 'presencePenalty',\n      });\n    }\n\n    if (stopSequences != null) {\n      warnings.push({\n        type: 'unsupported-setting',\n        setting: 'stopSequences',\n      });\n    }\n\n    if (\n      responseFormat != null &&\n      responseFormat.type === 'json' &&\n      responseFormat.schema != null\n    ) {\n      warnings.push({\n        type: 'unsupported-setting',\n        setting: 'responseFormat',\n        details: 'JSON response format schema is not supported',\n      });\n    }\n\n    // convert ai sdk messages to xai format\n    const { messages, warnings: messageWarnings } =\n      convertToXaiChatMessages(prompt);\n    warnings.push(...messageWarnings);\n\n    // prepare tools for xai\n    const {\n      tools: xaiTools,\n      toolChoice: xaiToolChoice,\n      toolWarnings,\n    } = prepareTools({\n      tools,\n      toolChoice,\n    });\n    warnings.push(...toolWarnings);\n\n    const baseArgs = {\n      // model id\n      model: this.modelId,\n\n      // standard generation settings\n      max_tokens: maxOutputTokens,\n      temperature,\n      top_p: topP,\n      seed,\n      reasoning_effort: options.reasoningEffort,\n\n      // response format\n      response_format:\n        responseFormat?.type === 'json'\n          ? responseFormat.schema != null\n            ? {\n                type: 'json_schema',\n                json_schema: {\n                  name: responseFormat.name ?? 'response',\n                  schema: responseFormat.schema,\n                  strict: true,\n                },\n              }\n            : { type: 'json_object' }\n          : undefined,\n\n      // search parameters\n      search_parameters: options.searchParameters\n        ? {\n            mode: options.searchParameters.mode,\n            return_citations: options.searchParameters.returnCitations,\n            from_date: options.searchParameters.fromDate,\n            to_date: options.searchParameters.toDate,\n            max_search_results: options.searchParameters.maxSearchResults,\n            sources: options.searchParameters.sources?.map(source => ({\n              type: source.type,\n              ...(source.type === 'web' && {\n                country: source.country,\n                excluded_websites: source.excludedWebsites,\n                allowed_websites: source.allowedWebsites,\n                safe_search: source.safeSearch,\n              }),\n              ...(source.type === 'x' && {\n                x_handles: source.xHandles,\n              }),\n              ...(source.type === 'news' && {\n                country: source.country,\n                excluded_websites: source.excludedWebsites,\n                safe_search: source.safeSearch,\n              }),\n              ...(source.type === 'rss' && {\n                links: source.links,\n              }),\n            })),\n          }\n        : undefined,\n\n      // messages in xai format\n      messages,\n\n      // tools in xai format\n      tools: xaiTools,\n      tool_choice: xaiToolChoice,\n    };\n\n    return {\n      args: baseArgs,\n      warnings,\n    };\n  }\n\n  async doGenerate(\n    options: Parameters<LanguageModelV2['doGenerate']>[0],\n  ): Promise<Awaited<ReturnType<LanguageModelV2['doGenerate']>>> {\n    const { args: body, warnings } = await this.getArgs(options);\n\n    const {\n      responseHeaders,\n      value: response,\n      rawValue: rawResponse,\n    } = await postJsonToApi({\n      url: `${this.config.baseURL ?? 'https://api.x.ai/v1'}/chat/completions`,\n      headers: combineHeaders(this.config.headers(), options.headers),\n      body,\n      failedResponseHandler: xaiFailedResponseHandler,\n      successfulResponseHandler: createJsonResponseHandler(\n        xaiChatResponseSchema,\n      ),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch,\n    });\n\n    const choice = response.choices[0];\n    const content: Array<LanguageModelV2Content> = [];\n\n    // extract text content\n    if (choice.message.content != null && choice.message.content.length > 0) {\n      let text = choice.message.content;\n\n      // skip if this content duplicates the last assistant message\n      const lastMessage = body.messages[body.messages.length - 1];\n      if (lastMessage?.role === 'assistant' && text === lastMessage.content) {\n        text = '';\n      }\n\n      if (text.length > 0) {\n        content.push({ type: 'text', text });\n      }\n    }\n\n    // extract reasoning content\n    if (\n      choice.message.reasoning_content != null &&\n      choice.message.reasoning_content.length > 0\n    ) {\n      content.push({\n        type: 'reasoning',\n        text: choice.message.reasoning_content,\n      });\n    }\n\n    // extract tool calls\n    if (choice.message.tool_calls != null) {\n      for (const toolCall of choice.message.tool_calls) {\n        content.push({\n          type: 'tool-call',\n          toolCallId: toolCall.id,\n          toolName: toolCall.function.name,\n          input: toolCall.function.arguments,\n        });\n      }\n    }\n\n    // extract citations\n    if (response.citations != null) {\n      for (const url of response.citations) {\n        content.push({\n          type: 'source',\n          sourceType: 'url',\n          id: this.config.generateId(),\n          url,\n        });\n      }\n    }\n\n    return {\n      content,\n      finishReason: mapXaiFinishReason(choice.finish_reason),\n      usage: {\n        inputTokens: response.usage.prompt_tokens,\n        outputTokens: response.usage.completion_tokens,\n        totalTokens: response.usage.total_tokens,\n        reasoningTokens:\n          response.usage.completion_tokens_details?.reasoning_tokens ??\n          undefined,\n      },\n      request: { body },\n      response: {\n        ...getResponseMetadata(response),\n        headers: responseHeaders,\n        body: rawResponse,\n      },\n      warnings,\n    };\n  }\n\n  async doStream(\n    options: Parameters<LanguageModelV2['doStream']>[0],\n  ): Promise<Awaited<ReturnType<LanguageModelV2['doStream']>>> {\n    const { args, warnings } = await this.getArgs(options);\n    const body = {\n      ...args,\n      stream: true,\n      stream_options: {\n        include_usage: true,\n      },\n    };\n\n    const { responseHeaders, value: response } = await postJsonToApi({\n      url: `${this.config.baseURL ?? 'https://api.x.ai/v1'}/chat/completions`,\n      headers: combineHeaders(this.config.headers(), options.headers),\n      body,\n      failedResponseHandler: xaiFailedResponseHandler,\n      successfulResponseHandler:\n        createEventSourceResponseHandler(xaiChatChunkSchema),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch,\n    });\n\n    let finishReason: LanguageModelV2FinishReason = 'unknown';\n    const usage: LanguageModelV2Usage = {\n      inputTokens: undefined,\n      outputTokens: undefined,\n      totalTokens: undefined,\n    };\n    let isFirstChunk = true;\n    const contentBlocks: Record<string, { type: 'text' | 'reasoning' }> = {};\n    const lastReasoningDeltas: Record<string, string> = {};\n\n    const self = this;\n\n    return {\n      stream: response.pipeThrough(\n        new TransformStream<\n          ParseResult<z.infer<typeof xaiChatChunkSchema>>,\n          LanguageModelV2StreamPart\n        >({\n          start(controller) {\n            controller.enqueue({ type: 'stream-start', warnings });\n          },\n\n          transform(chunk, controller) {\n            // Emit raw chunk if requested (before anything else)\n            if (options.includeRawChunks) {\n              controller.enqueue({ type: 'raw', rawValue: chunk.rawValue });\n            }\n\n            if (!chunk.success) {\n              controller.enqueue({ type: 'error', error: chunk.error });\n              return;\n            }\n\n            const value = chunk.value;\n\n            // emit response metadata on first chunk\n            if (isFirstChunk) {\n              controller.enqueue({\n                type: 'response-metadata',\n                ...getResponseMetadata(value),\n              });\n              isFirstChunk = false;\n            }\n\n            // emit citations if present (they come in the last chunk according to docs)\n            if (value.citations != null) {\n              for (const url of value.citations) {\n                controller.enqueue({\n                  type: 'source',\n                  sourceType: 'url',\n                  id: self.config.generateId(),\n                  url,\n                });\n              }\n            }\n\n            // update usage if present\n            if (value.usage != null) {\n              usage.inputTokens = value.usage.prompt_tokens;\n              usage.outputTokens = value.usage.completion_tokens;\n              usage.totalTokens = value.usage.total_tokens;\n              usage.reasoningTokens =\n                value.usage.completion_tokens_details?.reasoning_tokens ??\n                undefined;\n            }\n\n            const choice = value.choices[0];\n\n            // update finish reason if present\n            if (choice?.finish_reason != null) {\n              finishReason = mapXaiFinishReason(choice.finish_reason);\n            }\n\n            // exit if no delta to process\n            if (choice?.delta == null) {\n              return;\n            }\n\n            const delta = choice.delta;\n            const choiceIndex = choice.index;\n\n            // process text content\n            if (delta.content != null && delta.content.length > 0) {\n              const textContent = delta.content;\n\n              // skip if this content duplicates the last assistant message\n              const lastMessage = body.messages[body.messages.length - 1];\n              if (\n                lastMessage?.role === 'assistant' &&\n                textContent === lastMessage.content\n              ) {\n                return;\n              }\n\n              const blockId = `text-${value.id || choiceIndex}`;\n\n              if (contentBlocks[blockId] == null) {\n                contentBlocks[blockId] = { type: 'text' };\n                controller.enqueue({\n                  type: 'text-start',\n                  id: blockId,\n                });\n              }\n\n              controller.enqueue({\n                type: 'text-delta',\n                id: blockId,\n                delta: textContent,\n              });\n            }\n\n            // process reasoning content\n            if (\n              delta.reasoning_content != null &&\n              delta.reasoning_content.length > 0\n            ) {\n              const blockId = `reasoning-${value.id || choiceIndex}`;\n\n              // skip if this reasoning content duplicates the last delta\n              if (lastReasoningDeltas[blockId] === delta.reasoning_content) {\n                return;\n              }\n              lastReasoningDeltas[blockId] = delta.reasoning_content;\n\n              if (contentBlocks[blockId] == null) {\n                contentBlocks[blockId] = { type: 'reasoning' };\n                controller.enqueue({\n                  type: 'reasoning-start',\n                  id: blockId,\n                });\n              }\n\n              controller.enqueue({\n                type: 'reasoning-delta',\n                id: blockId,\n                delta: delta.reasoning_content,\n              });\n            }\n\n            // process tool calls\n            if (delta.tool_calls != null) {\n              for (const toolCall of delta.tool_calls) {\n                // xai tool calls come in one piece (like mistral)\n                const toolCallId = toolCall.id;\n\n                controller.enqueue({\n                  type: 'tool-input-start',\n                  id: toolCallId,\n                  toolName: toolCall.function.name,\n                });\n\n                controller.enqueue({\n                  type: 'tool-input-delta',\n                  id: toolCallId,\n                  delta: toolCall.function.arguments,\n                });\n\n                controller.enqueue({\n                  type: 'tool-input-end',\n                  id: toolCallId,\n                });\n\n                controller.enqueue({\n                  type: 'tool-call',\n                  toolCallId,\n                  toolName: toolCall.function.name,\n                  input: toolCall.function.arguments,\n                });\n              }\n            }\n          },\n\n          flush(controller) {\n            for (const [blockId, block] of Object.entries(contentBlocks)) {\n              controller.enqueue({\n                type: block.type === 'text' ? 'text-end' : 'reasoning-end',\n                id: blockId,\n              });\n            }\n\n            controller.enqueue({ type: 'finish', finishReason, usage });\n          },\n        }),\n      ),\n      request: { body },\n      response: { headers: responseHeaders },\n    };\n  }\n}\n\n// XAI API Response Schemas\nconst xaiUsageSchema = z.object({\n  prompt_tokens: z.number(),\n  completion_tokens: z.number(),\n  total_tokens: z.number(),\n  completion_tokens_details: z\n    .object({\n      reasoning_tokens: z.number().nullish(),\n    })\n    .nullish(),\n});\n\nconst xaiChatResponseSchema = z.object({\n  id: z.string().nullish(),\n  created: z.number().nullish(),\n  model: z.string().nullish(),\n  choices: z.array(\n    z.object({\n      message: z.object({\n        role: z.literal('assistant'),\n        content: z.string().nullish(),\n        reasoning_content: z.string().nullish(),\n        tool_calls: z\n          .array(\n            z.object({\n              id: z.string(),\n              type: z.literal('function'),\n              function: z.object({\n                name: z.string(),\n                arguments: z.string(),\n              }),\n            }),\n          )\n          .nullish(),\n      }),\n      index: z.number(),\n      finish_reason: z.string().nullish(),\n    }),\n  ),\n  object: z.literal('chat.completion'),\n  usage: xaiUsageSchema,\n  citations: z.array(z.string().url()).nullish(),\n});\n\nconst xaiChatChunkSchema = z.object({\n  id: z.string().nullish(),\n  created: z.number().nullish(),\n  model: z.string().nullish(),\n  choices: z.array(\n    z.object({\n      delta: z.object({\n        role: z.enum(['assistant']).optional(),\n        content: z.string().nullish(),\n        reasoning_content: z.string().nullish(),\n        tool_calls: z\n          .array(\n            z.object({\n              id: z.string(),\n              type: z.literal('function'),\n              function: z.object({\n                name: z.string(),\n                arguments: z.string(),\n              }),\n            }),\n          )\n          .nullish(),\n      }),\n      finish_reason: z.string().nullish(),\n      index: z.number(),\n    }),\n  ),\n  usage: xaiUsageSchema.nullish(),\n  citations: z.array(z.string().url()).nullish(),\n});\n","import {\n  OpenAICompatibleImageModel,\n  ProviderErrorStructure,\n} from '@ai-sdk/openai-compatible';\nimport {\n  ImageModelV2,\n  LanguageModelV2,\n  NoSuchModelError,\n  ProviderV2,\n} from '@ai-sdk/provider';\nimport {\n  FetchFunction,\n  generateId,\n  loadApiKey,\n  withoutTrailingSlash,\n} from '@ai-sdk/provider-utils';\nimport { XaiChatLanguageModel } from './xai-chat-language-model';\nimport { XaiChatModelId } from './xai-chat-options';\nimport { XaiErrorData, xaiErrorDataSchema } from './xai-error';\nimport { XaiImageModelId } from './xai-image-settings';\n\nconst xaiErrorStructure: ProviderErrorStructure<XaiErrorData> = {\n  errorSchema: xaiErrorDataSchema,\n  errorToMessage: data => data.error.message,\n};\n\nexport interface XaiProvider extends ProviderV2 {\n  /**\nCreates an Xai chat model for text generation.\n   */\n  (modelId: XaiChatModelId): LanguageModelV2;\n\n  /**\nCreates an Xai language model for text generation.\n   */\n  languageModel(modelId: XaiChatModelId): LanguageModelV2;\n\n  /**\nCreates an Xai chat model for text generation.\n   */\n  chat: (modelId: XaiChatModelId) => LanguageModelV2;\n\n  /**\nCreates an Xai image model for image generation.\n   */\n  image(modelId: XaiImageModelId): ImageModelV2;\n\n  /**\nCreates an Xai image model for image generation.\n   */\n  imageModel(modelId: XaiImageModelId): ImageModelV2;\n}\n\nexport interface XaiProviderSettings {\n  /**\nBase URL for the xAI API calls.\n     */\n  baseURL?: string;\n\n  /**\nAPI key for authenticating requests.\n   */\n  apiKey?: string;\n\n  /**\nCustom headers to include in the requests.\n   */\n  headers?: Record<string, string>;\n\n  /**\nCustom fetch implementation. You can use it as a middleware to intercept requests,\nor to provide a custom fetch implementation for e.g. testing.\n  */\n  fetch?: FetchFunction;\n}\n\nexport function createXai(options: XaiProviderSettings = {}): XaiProvider {\n  const baseURL = withoutTrailingSlash(\n    options.baseURL ?? 'https://api.x.ai/v1',\n  );\n  const getHeaders = () => ({\n    Authorization: `Bearer ${loadApiKey({\n      apiKey: options.apiKey,\n      environmentVariableName: 'XAI_API_KEY',\n      description: 'xAI API key',\n    })}`,\n    ...options.headers,\n  });\n\n  const createLanguageModel = (modelId: XaiChatModelId) => {\n    return new XaiChatLanguageModel(modelId, {\n      provider: 'xai.chat',\n      baseURL,\n      headers: getHeaders,\n      generateId,\n      fetch: options.fetch,\n    });\n  };\n\n  const createImageModel = (modelId: XaiImageModelId) => {\n    return new OpenAICompatibleImageModel(modelId, {\n      provider: 'xai.image',\n      url: ({ path }) => `${baseURL}${path}`,\n      headers: getHeaders,\n      fetch: options.fetch,\n      errorStructure: xaiErrorStructure,\n    });\n  };\n\n  const provider = (modelId: XaiChatModelId) => createLanguageModel(modelId);\n\n  provider.languageModel = createLanguageModel;\n  provider.chat = createLanguageModel;\n  provider.textEmbeddingModel = (modelId: string) => {\n    throw new NoSuchModelError({ modelId, modelType: 'textEmbeddingModel' });\n  };\n  provider.imageModel = createImageModel;\n  provider.image = createImageModel;\n\n  return provider;\n}\n\nexport const xai = createXai();\n"]}