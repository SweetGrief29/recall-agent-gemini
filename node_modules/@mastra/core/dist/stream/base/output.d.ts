import type { ReadableStream } from 'stream/web';
import { TransformStream } from 'stream/web';
import type { SharedV2ProviderMetadata, LanguageModelV2CallWarning } from '@ai-sdk/provider-v5';
import type { Span } from '@opentelemetry/api';
import type { TelemetrySettings } from 'ai-v5';
import { MessageList } from '../../agent/message-list/index.js';
import type { AIV5Type } from '../../agent/message-list/types.js';
import type { TracingContext } from '../../ai-tracing/index.js';
import { MastraBase } from '../../base.js';
import type { OutputProcessor } from '../../processors/index.js';
import type { ProcessorRunnerMode } from '../../processors/runner.js';
import { ProcessorRunner } from '../../processors/runner.js';
import type { ScorerRunInputForAgent, ScorerRunOutputForAgent } from '../../scores/index.js';
import type { ConsumeStreamOptions } from '../aisdk/v5/compat/index.js';
import { AISDKV5OutputStream } from '../aisdk/v5/output.js';
import type { ChunkType, StepBufferItem } from '../types.js';
import type { InferSchemaOutput, OutputSchema, PartialSchemaOutput } from './schema.js';
export declare class JsonToSseTransformStream extends TransformStream<unknown, string> {
    constructor();
}
type MastraModelOutputOptions<OUTPUT extends OutputSchema = undefined> = {
    runId: string;
    rootSpan?: Span;
    telemetry_settings?: TelemetrySettings;
    toolCallStreaming?: boolean;
    onFinish?: (event: Record<string, any>) => Promise<void> | void;
    onStepFinish?: (event: Record<string, any>) => Promise<void> | void;
    includeRawChunks?: boolean;
    output?: OUTPUT;
    outputProcessors?: OutputProcessor[];
    outputProcessorRunnerMode?: ProcessorRunnerMode;
    returnScorerData?: boolean;
    tracingContext?: TracingContext;
};
export declare class MastraModelOutput<OUTPUT extends OutputSchema = undefined> extends MastraBase {
    #private;
    /**
     * Unique identifier for this execution run.
     */
    runId: string;
    /**
     * The processor runner for this stream.
     */
    processorRunner?: ProcessorRunner;
    private outputProcessorRunnerMode;
    /**
     * The message list for this stream.
     */
    messageList: MessageList;
    /**
     * Trace ID used on the execution (if the execution was traced).
     */
    traceId?: string;
    constructor({ model: _model, stream, messageList, options, }: {
        model: {
            modelId: string | undefined;
            provider: string | undefined;
            version: 'v1' | 'v2';
        };
        stream: ReadableStream<ChunkType<OUTPUT>>;
        messageList: MessageList;
        options: MastraModelOutputOptions<OUTPUT>;
    });
    /**
     * Resolves to the complete text response after streaming completes.
     */
    get text(): Promise<string>;
    /**
     * Resolves to complete reasoning text for models that support reasoning.
     */
    get reasoning(): Promise<string>;
    get reasoningText(): Promise<string | undefined>;
    get reasoningDetails(): Promise<{
        type: string;
        text: string;
        providerMetadata: SharedV2ProviderMetadata;
    }[]>;
    get sources(): Promise<any[]>;
    get files(): Promise<any[]>;
    get steps(): Promise<StepBufferItem[]>;
    teeStream(): ReadableStream<ChunkType<OUTPUT>>;
    /**
     * Stream of all chunks. Provides complete control over stream processing.
     */
    get fullStream(): ReadableStream<ChunkType<OUTPUT>>;
    /**
     * Resolves to the reason generation finished.
     */
    get finishReason(): Promise<string | undefined>;
    /**
     * Resolves to array of all tool calls made during execution.
     */
    get toolCalls(): Promise<any[]>;
    /**
     * Resolves to array of all tool execution results.
     */
    get toolResults(): Promise<any[]>;
    /**
     * Resolves to token usage statistics including inputTokens, outputTokens, and totalTokens.
     */
    get usage(): Promise<Record<string, number>>;
    /**
     * Resolves to array of all warnings generated during execution.
     */
    get warnings(): Promise<LanguageModelV2CallWarning[]>;
    /**
     * Resolves to provider metadata generated during execution.
     */
    get providerMetadata(): Promise<Record<string, any> | undefined>;
    /**
     * Resolves to the complete response from the model.
     */
    get response(): Promise<Record<string, any>>;
    /**
     * Resolves to the complete request sent to the model.
     */
    get request(): Promise<Record<string, any>>;
    /**
     * Resolves to an error if an error occurred during streaming.
     */
    get error(): Error | string | {
        message: string;
        stack: string;
    } | undefined;
    updateUsageCount(usage: Record<string, number>): void;
    populateUsageCount(usage: Record<string, number>): void;
    consumeStream(options?: ConsumeStreamOptions): Promise<void>;
    /**
     * Returns complete output including text, usage, tool calls, and all metadata.
     */
    getFullOutput(): Promise<{
        traceId: string | undefined;
        scoringData?: {
            input: Omit<ScorerRunInputForAgent, "runId">;
            output: ScorerRunOutputForAgent;
        } | undefined;
        text: string;
        usage: Record<string, number>;
        steps: StepBufferItem[];
        finishReason: string | undefined;
        warnings: LanguageModelV2CallWarning[];
        providerMetadata: Record<string, any> | undefined;
        request: Record<string, any>;
        reasoning: string;
        reasoningText: string | undefined;
        toolCalls: any[];
        toolResults: any[];
        sources: any[];
        files: any[];
        response: Record<string, any>;
        totalUsage: Record<string, number>;
        object: Awaited<InferSchemaOutput<OUTPUT>>;
        error: string | Error | {
            message: string;
            stack: string;
        } | undefined;
        tripwire: boolean;
        tripwireReason: string;
    }>;
    /**
     * The tripwire flag is set when the stream is aborted due to an output processor blocking the content.
     */
    get tripwire(): boolean;
    /**
     * The reason for the tripwire.
     */
    get tripwireReason(): string;
    /**
     * The total usage of the stream.
     */
    get totalUsage(): Promise<Record<string, number>>;
    get content(): Promise<({
        type: "text";
        text: string;
        providerMetadata?: AIV5Type.ProviderMetadata;
    } | {
        type: "reasoning";
        text: string;
        providerMetadata?: AIV5Type.ProviderMetadata;
    } | ({
        type: "source";
    } & import("@ai-sdk/provider-v5").LanguageModelV2Source) | {
        type: "file";
        file: AIV5Type.Experimental_GeneratedImage;
        providerMetadata?: AIV5Type.ProviderMetadata;
    } | ({
        type: "tool-call";
    } & (AIV5Type.TypedToolCall<any> & {
        providerMetadata?: AIV5Type.ProviderMetadata;
    })) | ({
        type: "tool-result";
    } & (AIV5Type.TypedToolResult<any> & {
        providerMetadata?: AIV5Type.ProviderMetadata;
    })) | ({
        type: "tool-error";
    } & (AIV5Type.TypedToolError<any> & {
        providerMetadata?: AIV5Type.ProviderMetadata;
    })))[]>;
    /**
     * Other output stream formats.
     */
    get aisdk(): {
        /**
         * The AI SDK v5 output stream format.
         */
        v5: AISDKV5OutputStream<OUTPUT>;
    };
    /**
     * Stream of valid JSON chunks. The final JSON result is validated against the output schema when the stream ends.
     *
     * @example
     * ```typescript
     * const stream = await agent.streamVNext("Extract data", {
     *   output: z.object({ name: z.string(), age: z.number() })
     * });
     * // partial json chunks
     * for await (const data of stream.objectStream) {
     *   console.log(data); // { name: 'John' }, { name: 'John', age: 30 }
     * }
     * ```
     */
    get objectStream(): ReadableStream<PartialSchemaOutput<OUTPUT>>;
    /**
     * Stream of individual array elements when output schema is an array type.
     */
    get elementStream(): ReadableStream<InferSchemaOutput<OUTPUT> extends Array<infer T> ? T : never>;
    /**
     * Stream of only text content, filtering out metadata and other chunk types.
     */
    get textStream(): ReadableStream<string>;
    /**
     * Resolves to the complete object response from the model. Validated against the 'output' schema when the stream ends.
     *
     * @example
     * ```typescript
     * const stream = await agent.streamVNext("Extract data", {
     *   output: z.object({ name: z.string(), age: z.number() })
     * });
     * // final validated json
     * const data = await stream.object // { name: 'John', age: 30 }
     * ```
     */
    get object(): Promise<InferSchemaOutput<OUTPUT>>;
    /** @internal */
    _getImmediateToolCalls(): any[];
    /** @internal */
    _getImmediateToolResults(): any[];
    /** @internal */
    _getImmediateText(): string;
    /** @internal */
    _getImmediateUsage(): Record<string, number>;
    /** @internal */
    _getImmediateWarnings(): LanguageModelV2CallWarning[];
    /** @internal */
    _getImmediateFinishReason(): string | undefined;
}
export {};
//# sourceMappingURL=output.d.ts.map